<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2024 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-linear-indep-concepts" xmlns:xi="http://www.w3.org/2001/XInclude">

<title>Concepts</title>

<introduction><assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-linear-indep-concepts-reduce-span" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-concepts-reduce-span" /></em></li>
<li><xref ref="subsection-linear-indep-concepts-linear-dep-indep" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-concepts-linear-dep-indep" /></em></li>
<li><xref ref="subsection-linear-indep-concepts-one-or-two" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-concepts-one-or-two" /></em></li>
<li><xref ref="subsection-linear-indep-concepts-in-Rn" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-concepts-in-Rn" /></em></li>
</ul></p></assemblage></introduction>

<introduction>

	<p>
	<xref ref="proposition-subspaces-span-subspace-full">Statement</xref>
	of
	<xref ref="proposition-subspaces-span-subspace" />
	guarantees that every vector space has a spanning set.
	To prove this statement, we verified that every vector space is trivially generated by itself, <ie /> <m>V = \Span V</m>.
	</p>

	<p>
	But this doesn't give us a useful description of the vector space <m>V</m>,
	it basically just says <q>to build all the vectors in <m>V</m> out of some vectors in <m>V</m>, use all the vectors in <m>V</m>.</q>
	The point of a spanning set is to give you a set of building-block vectors that can be used to construct every other vector in the space through linear combinations.
	To make an analogy to chemistry, the vectors in a spanning set act like <em>atoms</em>, and linear combinations are then like <em>molecules</em> built out of different combinations of different quantities of those atoms.
	So we would like our set of building blocks to be as simple as possible <mdash />
	that is, we would like to get down to some sort of <em>optimal</em> description of a vector space in terms of a spanning set.
	Linear dependence and independence are precisely the concepts we will need in order to judge whether we have such an optimal spanning set.
	</p>

</introduction>


<subsection xml:id="subsection-linear-indep-concepts-reduce-span">
<title>Reducing spanning sets</title>

<p>
Suppose we have a spanning set <m>S</m> for a space <m>V</m>,
so that <m>V = \Span S</m>.
This equality of spaces says that every vector in the space <m>V</m> can somehow be expressed as a linear combination of vectors in <m>S</m>.
</p>

<p>
Suppose further that one of the vectors in <m>S</m> can be expressed as a linear combination of some of the others, say
<md><mrow tag="star" xml:id="equation-linear-indep-concepts-exmp-w-lin-comb">
	\uvec{w} = k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_m \uvec{u}_m,
</mrow></md>
where each of <m>\uvec{w},\uvec{u}_1,\uvec{u}_2,\dotsc,\uvec{u}_m</m> is a vector in <m>S</m>.
</p>

<aside><title>Note</title><p>
	This was basically the situation for <xref ref="activity-linear-indep-motivation" />, where <m>\uvec{v}_2</m> played the role of <m>\uvec{w}</m>.
</p></aside>

<p>
<em>Then <m>\uvec{w}</m> is not actually needed when expressing vectors in <m>V</m> as linear combinations of vectors in <m>S</m>.</em>
</p>

<p>
Indeed, consider a vector <m>\uvec{x}</m> in <m>V</m> expressed as a linear combination of vectors in <m>S</m>, including <m>\uvec{w}</m>, say
<me> \uvec{x} = c\uvec{w} + c_1\uvec{v}_1 + c_2\uvec{v}_2 + \dotsb + k_m \uvec{v}_n </me>,
where each of <m>\uvec{v}_1,\uvec{v}_2,\dotsc,\uvec{v}_n</m> is a vector in <m>S</m>.
But then we could substitute the expression in <xref ref="equation-linear-indep-concepts-exmp-w-lin-comb" /> for <m>\uvec{w}</m> to obtain
<md>
	<mrow>
		\uvec{x}
		\amp= c(k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_m \uvec{u}_m)
		+ c_1\uvec{v}_1 + c_2\uvec{v}_2 + \dotsb + k_m \uvec{v}_n
	</mrow><mrow>
		\amp= ck_1\uvec{u}_1 + ck_2\uvec{u}_2 + \dotsb + ck_m \uvec{u}_m
		+ c_1\uvec{v}_1 + c_2\uvec{v}_2 + \dotsb + k_m \uvec{v}_n.
	</mrow>
</md>
Here, each of the <m>\uvec{u}_i</m> vectors and the <m>\uvec{v}_j</m> vectors are in <m>S</m>,
making this an expression for <m>\uvec{x}</m> as a linear combination of vectors in <m>S</m> <em>but not including <m>\uvec{w}</m></em>.
</p>

<p>
If <m>\uvec{w}</m> isn't needed to express vectors in <m>V</m> as linear combinations of vectors in <m>S</m>,
then we should have <m>\Span S = \Span S'</m>, where <m>S'</m> is the set of all vectors in <m>S</m> <em>except</em> <m>\uvec{w}</m>.
That is, we can discard <m>\uvec{w}</m> from the spanning set for <m>V</m>, and <em>still</em> have a spanning set.
</p>

<aside><title>See</title><p>
	<xref ref="lemma-linear-indep-span-minus-one" /> in <xref ref="subsection-linear-indep-theory-vs-span" />.
</p></aside>

<p>
This pattern will help us reduce down to some sort of <em>optimal</em> spanning set:
we can keep removing these redundant spanning vectors that are linear combinations of other spanning vectors until there are none left.
</p>

</subsection>


<subsection xml:id="subsection-linear-indep-concepts-linear-dep-indep">
<title>Linear dependence and independence</title>

<p>
A set of vectors is called <term>linearly dependent</term> precisely when it is non-optimal as a spanning set in the way described above:
when one of the vectors in the set is a linear combination of others in the set.
However, it is usually not obvious that some vector is redundant in this way <mdash />
checking each vector in turn is tedious, and also would not be a very convenient way to reason with the concept of linear dependence in the abstract.
</p>

<p>
However, having an expression for a vector <m>\uvec{w}</m> as a linear combination of other vectors <m>\uvec{u}_1,\uvec{u}_2,\dotsc,\uvec{u}_m</m>, such as
<md><mrow tag="dstar" xml:id="equation-linear-indep-concepts-exmp-w-lin-comb-again">
	\uvec{w} = k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_m \uvec{u}_m
</mrow></md>,
is equivalent to having a nontrivial linear combination of these vectors equal to the zero vector:
<md><mrow tag="tstar" xml:id="equation-linear-indep-concepts-exmp-nontrivial-lin-comb-eq-zerovec">
	k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_m \uvec{u}_m + (-1)\uvec{w}  = \zerovec
</mrow></md>.
And vice versa, since if we have a nontrivial linear combination of these vectors that results in the zero vector, say
<me> k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_m \uvec{u}_m + k\uvec{w}  = \zerovec </me>,
and the coefficient <m>k</m> on <m>\uvec{w}</m> is nonzero, then we can rearrange to get
<me>
	\uvec{w}
	= -\frac{k_1}{k}\uvec{u}_1 + \left(-\frac{k_2}{k}\right)\uvec{u}_2 + \dotsb + \left(-\frac{k_m}{k}\right) \uvec{u}_m
</me>,
an expression for <m>\uvec{w}</m> as a linear combination of the others.
</p>

<p>
Again, the advantage of checking for linear combinations equal to the zero vector is that in a collection of vectors <m>S</m>,
we usually don't know ahead of time which one will be the odd one out
(that is, which one will play the role of <m>\uvec{w}</m> as above).
In a nontrivial linear combination equalling <m>\zerovec</m>,
we can take as the redundant vector <m>\uvec{w}</m> any of the vectors whose coefficient is nonzero
(which is required to perform the step of dividing by <m>k</m> in the algebra isolating <m>\uvec{w}</m> above).
</p>

<p>
Now, we can always take the <em>trivial</em> linear combination, where all coefficients are <m>0</m>, to get a result of <m>\zerovec</m>.
But if this is the <em>only</em> linear combination of a set of vectors by which we can get <m>\zerovec</m> as a result,
then <em>none</em> of the vectors can act as the redundant <m>\uvec{w}</m> as above, because an expression like
<xref ref="equation-linear-indep-concepts-exmp-w-lin-comb-again" />
<em>always</em> leads to an equality like
<xref ref="equation-linear-indep-concepts-exmp-nontrivial-lin-comb-eq-zerovec" />, involving a nontrivial linear combination.
</p>

<p> This logic leads to the Test for Linear Dependence/Independence. </p>

<algorithm xml:id="procedure-linear-indep-concepts-test-for-dep-indep">
	<title>Test for Linear Dependence/Independence</title>
	<statement>

		<p>
		To test whether vectors
		<m>\uvec{v}_1,\uvec{v}_2,\dotsc,\uvec{v}_m</m>
		are linearly dependent/independent,
		solve the homogeneous vector equation
		<me> k_1\uvec{v}_1 + k_2\uvec{v}_2 + \dotsb + k_m\uvec{v}_m = \zerovec </me>
		in the (scalar) variables <m>k_1,k_2,\dotsc,k_m</m>.
		</p>

		<p>
		If this vector equation has a nontrivial solution for these coefficient variables,
		then the vectors
		<m>\uvec{v}_1,\uvec{v}_2,\dotsc,\uvec{v}_m</m>
		are linearly dependent.
		</p>

		<p>
		Otherwise, if this vector equation has <em>only</em> the trivial solution
		<m>k_1=0,k_2=0,\dotsc,k_m=0</m>,
		then the vectors
		<m>\uvec{v}_1,\uvec{v}_2,\dotsc,\uvec{v}_m</m>
		are linearly independent.
		</p>

	</statement>
</algorithm>

</subsection>


<subsection xml:id="subsection-linear-indep-concepts-one-or-two">
<title>Linear dependence and independence of just one or two vectors</title>

<p>
For a pair <m>\uvec{u},\uvec{v}</m> of vectors to be linearly dependent, one must be a linear combination of the other.
But a linear combination of one vector is just a scalar multiple, and so <em>a pair of vectors is linearly dependent if one is a scalar multiple of the other</em>.
If both vectors are nonzero, that scalar must also be nonzero and so could be shifted to the other side as its reciprocal:
<md><mrow>
	\uvec{u} \amp = k\uvec{v} \amp
	\amp\iff \amp
	\frac{1}{k}\uvec{u} \amp= \uvec{v} \amp
	\amp \text{(for } k\neq 0 \text{)}
</mrow></md>.
So nonzero vectors <em><m>\uvec{u},\uvec{v}</m> are linearly dependent if and only if each is a scalar multiple of the other</em>.
In <m>\R^n</m>, we would have called such vectors <term>parallel</term>.
</p>

<p>
What about a set containing a single vector?
Our motivation for creating the concept of linear dependence/independence was to measure whether a spanning set contained redundent information or whether it was somehow <q>optimal.</q>
A spanning set consisting of a single nonzero vector cannot be reduced to a smaller spanning set, so it is already optimal and we should refer to that spanning set as linearly independent.
This coincides with the result of the test for linear dependence/independence for a single vector <m>\uvec{v}</m>:
if <m>\uvec{v}</m> is nonzero, then there are no nontrivial solutions to the vector equation <m>k\uvec{v} = \zerovec</m>.
</p>

<aside><title>See</title><p>
	<xref ref="proposition-abstract-vec-spaces-basic-vec-props-zero-scalar-mul-eq">Rule</xref>
	of
	<xref ref="proposition-abstract-vec-spaces-basic-vec-props" />.
</p></aside>

<p>
But what about the zero vector by itself?
Scalar multiples of <m>\zerovec</m> remain <m>\zerovec</m>, so <m>\Span\{\zerovec\}</m> is the trivial vector space consisting of just the zero vector.
Is <m>\{\zerovec\}</m> an optimal spanning set for the trivial space, or can it be reduced further?
By convention, we also consider <m>\Span\{\}</m> to be the trivial vector space
(where <m>\{\}</m> represents a set containing <em>no</em> vectors, called the <term>empty set</term>),
because we always want the <m>\Span</m> process to return a subspace of the vector space in which we're working.
So the spanning set <m>\{\zerovec\}</m> <em>can</em> be reduced to the empty set, and still span the same space.
This line of reasoning leads us to consider the set of vectors containing only the zero vector to be linearly dependent.
</p>

<aside><title>Note</title><p> We should also consider the empty set to be linearly independent, since it cannot be reduced. </p></aside>

</subsection>


<subsection xml:id="subsection-linear-indep-concepts-in-Rn">
<title>Linear dependence and independence in <m>\R^n</m></title>

<paragraphs><title>Independent directions</title>

	<p>
	In <xref ref="subsection-subspaces-concepts-of-Rn" />, we discussed how a nonzero vector in <m>\R^n</m> spans a line through the origin.
	If a second vector is linearly dependent with the vector spanning the line, then as discussed in the previous subsection
	(<xref ref="subsection-linear-indep-concepts-one-or-two" />), that second vector must be parallel with the first, hence parallel with the line.
	To get something linearly independent, we need to branch off in a new direction from the line.
	</p>

	<image label="image-linear-indep-concepts-linear-indep-2vecs" width="72.3%">
		<!-- description gets inserted as alt text in html img tag -->
		<description>Diagram of linear independence in the case of two vectors</description>
		<latex-image><xi:include href="concepts.d/linear-indep-2vecs.tex" parse="text" /></latex-image>
	</image>

	<p>
	In <xref ref="subsection-subspaces-concepts-of-Rn" />, we also discussed how a pair of nonzero, nonparallel vectors in <m>\R^n</m> span a plane through the origin.
	If a third vector is linearly dependent with those two spanning vectors, it is somehow a linear combination of them and so lies in that plane.
	To get something linearly independent, we need to branch off in a new direction from that plane.
	</p>

	<image label="image-linear-indep-concepts-linear-indep-3vecs" width="79.7%">
		<!-- description gets inserted as alt text in html img tag -->
		<description>Diagram of linear independence in the case of three vectors</description>
		<latex-image><xi:include href="concepts.d/linear-indep-3vecs.tex" parse="text" /></latex-image>
	</image>

	<p>
	This idea that we can enlarge an independent set by including a new vector in a new direction works in abstract vector spaces as well, as we will see in
	<xref ref="proposition-linear-indep-expand-indep" />
	in
	<xref ref="subsection-linear-indep-theory-vs-span" />.
	</p>
</paragraphs>

<paragraphs><title>Maximum number of linearly independent vectors</title>

	<p>
	In <xref ref="activity-linear-indep-max-indep-in-R2-R3" />, we considered the possible sizes of linearly independent sets in <m>\R^2</m> and <m>\R^3</m>.
	We can certainly have two linearly independent vectors in <m>\R^2</m>, since clearly the standard basis vectors <m>\uvec{e}_1</m> and <m>\uvec{e}_2</m> form a linearly independent set in <m>\R^2</m>.
	Could we have three?
	Two linearly independent vectors would have to be nonparallel, and so they would have to span a plane <mdash />
	<ie /> they would have to span the <m>entire</m> plane.
	Geometrically, a third linearly independent vector would have to branch off in a <q>new direction,</q> as in our discussion above.
	But in <m>\R^2</m> there is no new third direction in which to head <mdash />
	we can't have a vector breaking up out of the plane <q>into the third dimension,</q> because there is no third dimension available in <m>\R^2</m>.
	Algebraically, if we had three vectors <m>\uvec{u}=(u_1,u_2)</m>, <m>\uvec{v}=(v_1,v_2)</m>, <m>\uvec{w}=(w_1,w_2)</m> in <m>\R^2</m> and attempted the test for dependence/independence,
	we would start by setting up the vector equation
	<me> k_1\uvec{u} + k_2\uvec{v} + k_3\uvec{w} = \zerovec </me>.
	Combining the linear combination on the left back into one vector, and comparing coordinates on either side, we would obtain linear system
	<me>
		\begin{sysofeqns}{rcrcrcr}
			u_1 k_1 \amp + \amp v_1 k_2 \amp + \amp w_1 k_3 \amp = \amp 0 \text{,} \\
			u_2 k_1 \amp + \amp v_2 k_2 \amp + \amp w_2 k_3 \amp = \amp 0 \text{,}
		\end{sysofeqns}
	</me>
	in the unknown coefficients <m>k_1,k_2,k_3</m>.
	Because there are only two equations, the reduced form for the coefficient matrix for this system can have no more than two leading ones,
	so it requires at least one parameter to solve, which means there are nontrivial solutions.
	So three vectors in <m>\R^2</m> can never by linearly independent.
	</p>

	<p>
	We come to a similar conclusion in <m>\R^3</m> using both geometric and algebraic reasoning <mdash />
	three independent vectors in <m>\R^3</m> is certainly possible
	(for example, the standard basis vectors),
	but a set of four vectors in <m>\R^3</m> can never be linearly independent.
	Geometrically, once you have three independent vectors pointing in three <q>independent directions,</q>
	there is no new direction in <m>\R^3</m> for a fourth independent vector to take.
	Algebraically, we could set up the test for independence with four vectors in <m>\R^3</m> and it would lead to a homogeneous system of three equations (one for each coordinate) in four variables (one unknown coefficient for each vector).
	Since the system would have more variables than equations, it would require parameters to solve, leading to nontrivial solutions.
	</p>

	<p>
	And the pattern continues in higher dimensions <mdash />
	a collection of more than four vectors in <m>\R^4</m> must be linearly dependent, a collection of more than five vectors in <m>\R^5</m> must be linearly dependent, and so on.
	In fact, this pattern can also be found in abstract vectors spaces <mdash />
	see
	<xref ref="lemma-linear-indep-more-than-spanning-is-dep" />
	in
	<xref ref="subsection-linear-indep-theory-vs-span" />.
	And this pattern will help us transplant the idea of <term>dimension</term> from <m>\R^n</m> to abstract spaces.
	</p>

</paragraphs>

</subsection>

</section>
