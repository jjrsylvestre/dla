<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2024 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-linear-indep-examples">

<title>Examples</title>

<introduction><assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-linear-indep-examples-test" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-examples-test" /></em></li>
<li><xref ref="subsection-linear-indep-examples-standard" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-linear-indep-examples-standard" /></em></li>
</ul></p></assemblage></introduction>

<subsection xml:id="subsection-linear-indep-examples-test">
<title>Testing dependence/independence</title>

<p>
Here we will carry out examples of applying
the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>.
</p>

<example xml:id="example-linear-indep-test-R4">
	<title>Testing dependence/independence in <m>\R^n</m></title>

	<p>
	Are the vectors <m>(1,0,0,1),(1,1,0,-1),(2,1,0,0),(5,1,0,5)</m> in <m>\R^4</m> linearly dependent or independent?
	Set up the test:
	<me> k_1 (1,0,0,1) + k_2(1,1,0,-1) + k_3(2,1,0,0) + k_4(5,1,0,5) = (0,0,0,0) </me>.
	Notice how we have used the proper zero vector in this space on the right-hand side.
	On the left-hand side, we want to combine the expression into one vector so that we can compare with the zero vector.
	<md>
		<mrow> (k_1,0,0,k_1) + (k_2,k_2,0,-k_2) + (2k_3,k_3,0,0) + (5k_4,k_4,0,5k_4) \amp= (0,0,0,0) </mrow>
		<mrow> (k_1+k_2+2k_3+5k_4,k_2+k_3+k_4,0,k_1-k_2+5k_4) \amp= (0,0,0,0) </mrow>
	</md>
	Comparing components on either side, we obtain a system of four equations in the unknown scalars from the linear combination:
	<me>
		\begin{sysofeqns}{rcrcrcrcr}
			k_1 \amp + \amp k_2 \amp + \amp 2k_3 \amp + \amp 5k_4 \amp = \amp 0, \\
			    \amp   \amp k_2 \amp + \amp  k_3 \amp + \amp  k_4 \amp = \amp 0, \\
			    \amp   \amp     \amp   \amp      \amp   \amp    0 \amp = \amp 0, \\
			k_1 \amp - \amp k_2 \amp   \amp      \amp + \amp 5k_4 \amp = \amp 0. \\
		\end{sysofeqns}
	</me>
	Now we'll solve this homogeneous system by row reducing its coefficient matrix.
	<md><mrow xml:id="equation-linear-indep-examples-testing-R4-independence-homog-system" tag="star">
		\left[\begin{array}{rrrr}
			1 \amp 1 \amp 2 \amp 5 \\
			0 \amp 1 \amp 1 \amp 1 \\
			0 \amp 0 \amp 0 \amp 0 \\
			1 \amp -1 \amp 0 \amp 5
		\end{array}\right]
		\qquad\rowredarrow\qquad
		\begin{bmatrix}
			1  \amp  0  \amp  1  \amp  0 \\
			0  \amp  1  \amp  1  \amp  0 \\
			0  \amp  0  \amp  0  \amp  1 \\
			0 \amp 0 \amp 0 \amp 0
		\end{bmatrix}
	</mrow></md>
	Note that here it was not necessary to reduce all the way to RREF, as we are not actually interested in the solutions to this system <mdash />
	we only need to know whether there exist nontrivial solutions.
	From the reduced matrix, we can see that <m>k_3</m> is a free variable and would be assigned a parameter in the general solution.
	The necessity of a parameter means there are an infinite number of solutions, which in particular means there are nontrivial solutions.
	Therefore, this collection of vectors is <em>linearly dependent</em>.
	</p>

</example>

<remark><p>
	Notice how the vectors from <m>\R^4</m> that we were testing in the previous example ended up as columns in the coefficient matrix in
	<xref ref="equation-linear-indep-examples-testing-R4-independence-homog-system" tag="star"/>
	<mdash /> we saw a similar pattern in
	<xref ref="example-subspaces-check-in-span-in-R3" />
	(and in the other examples in <xref ref="subsection-subspaces-examples-in-span" />),
	where we tested whether a particular vector was in the span of some collection of vectors.
</p></remark>

<example><title>Testing dependence/independence in <m>\matrixring_{m \times n}(\R)</m></title><p><ol>

	<li>
		Consider the matrices in
		<xref ref="activity-linear-indep-using-indep-test-M2-1">Discovery</xref>.
		First we set up
		the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>.
		Again, we use the proper zero vector on the right-hand side,
		and then we combine the expression on the left-hand side into one vector so that we may compare against the zero vector.
		<md>
			<mrow>
				k_1\begin{bmatrix}1 \amp 0\\0 \amp 1\end{bmatrix} +
				k_2\begin{bmatrix}0 \amp 1\\1 \amp 0\end{bmatrix} +
				k_3\begin{bmatrix}0 \amp 0\\0 \amp 1\end{bmatrix}  \amp =
				\begin{bmatrix}0 \amp 0\\0 \amp 0\end{bmatrix}
			</mrow><mrow>
				\begin{bmatrix}k_1 \amp 0\\0 \amp k_1\end{bmatrix} +
				\begin{bmatrix}0 \amp k_2\\k_2 \amp 0\end{bmatrix} +
				\begin{bmatrix}0 \amp 0\\0 \amp k_3\end{bmatrix}  \amp =
				\begin{bmatrix}0 \amp 0\\0 \amp 0\end{bmatrix}
			</mrow><mrow>
				\begin{bmatrix}k_1 \amp k_2\\k_2 \amp k_1+k_3\end{bmatrix}  \amp =
				\begin{bmatrix}0 \amp 0\\0 \amp 0\end{bmatrix}
			</mrow>
		</md>
		There is no need to set up a system of equations here <mdash />
		we can see from comparing the top rows on either side that <m>k_1=0</m> and <m>k_2=0</m>.
		Then, from the <m>(2,2)</m> entries, we see that <m>k_1+k_3=0</m>.
		But since we already have <m>k_1=0</m>, we get <m>k_3=0</m> as well.
		So there is only the trivial solution, and these vectors are <em>linearly independent</em>.
	</li>

	<li>

		<p>
		Consider the matrices in
		<xref ref="activity-linear-indep-using-indep-test-M2-2">Discovery</xref>.
		Again, we start by setting up
		the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>
		using the appropriate zero vector.
		<me>
			k_1 \begin{bmatrix} 1 \amp 0 \\ 0 \amp 1 \end{bmatrix} +
			k_2 \left[\begin{array}{rr} 1 \amp 0 \\ 0 \amp -1 \end{array}\right] +
			k_3 \left[\begin{array}{rr} 3 \amp 0 \\ 0 \amp -2 \end{array}\right] =
			\begin{bmatrix} 0 \amp 0 \\ 0 \amp 0 \end{bmatrix}
		</me>
		As before, this will lead to a homogeneous system of equations in the unknown scalars <m>k_1,k_2,k_3</m>,
		and the coefficient matrix of this system will have the entries of the three vectors as columns:
		<me>
			\left[\begin{array}{rrr}
				1 \amp 1 \amp 3 \\
				0 \amp 0 \amp 0 \\
				0 \amp 0 \amp 0 \\
				1 \amp -1 \amp -2
			\end{array}\right]
			\qquad\rowredarrow\qquad
			\begin{bmatrix}
				1  \amp  0  \amp  1/2 \\
				0  \amp  1  \amp  5/2 \\
				0  \amp  0  \amp  0 \\
				0  \amp  0  \amp  0
			\end{bmatrix}
		</me>.
		From the reduced matrix, we see that <m>k_3</m> is a free variable and will be assigned a parameter in the general solution.
		The necessity of a parameter implies nontrivial solutions, so these vectors are <em>linearly dependent</em>.
		</p>

		<p>
		The reduced matrix can also be used to tell us exactly how these vectors are linearly dependent.
		Since <m>k_3</m> is free, we obtain a solution to the system for every possible value we assign to it.
		To get a simple nontrivial solution, let's set <m>k_3=1</m>.
		Then solving the equations represented by the nonzero rows of the reduced matrix gives us <m>k_1=-1/2</m> and <m>k_2=-5/2</m>.
		Putting these back into the vector equation from when we first set up
		the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>,
		we get
		<md>
			<mrow>
				\left(-\frac{1}{2}\right)\begin{bmatrix} 1 \amp 0 \\ 0 \amp 1 \end{bmatrix}
				+ \left(-\frac{5}{2}\right)\left[\begin{array}{rr} 1 \amp 0\\0 \amp -1 \end{array}\right]
				+ \left[\begin{array}{rr} 3 \amp 0 \\ 0 \amp -2 \end{array}\right]
				= \begin{bmatrix}0 \amp 0\\0 \amp 0\end{bmatrix}
			</mrow><mrow></mrow><mrow>
				\implies\quad
				\left[\begin{array}{rr} 3 \amp 0 \\ 0 \amp -2 \end{array}\right] =
				\frac{1}{2}\begin{bmatrix} 1 \amp 0\\0 \amp 1 \end{bmatrix} +
				\frac{5}{2}\left[\begin{array}{rr} 1 \amp 0 \\ 0 \amp -1 \end{array}\right]
			</mrow>
		</md>.
		From this we see exactly how one of the vectors in our collection can be expressed as a linear combination of others in the collection.
		</p>

	</li>

</ol></p></example>

<example><title>Testing dependence/independence in <m>\poly_n(\R)</m></title>

	<p>
	Consider the polynomials from
	<xref ref="activity-linear-indep-using-indep-test-P2">Discovery</xref>.
	Are they linearly dependent or independent? Set up the test, using the zero polynomial as the zero vector on the right-hand side:
	<me> k_1(1+x) + k_2(1+x^2) + k_3(2-x+3x^2) = 0 </me>.
	As usual, we simplify the linear combination on the left-hand side into one vector.
	Here, this means collecting like terms.
	<me> (k_1+k_2+2k_3) + (k_1-k_3)x + (k_2+3k_3)x^2 = 0 </me>.
	The polynomial on the left can only be equal to the zero polynomial if all its coefficients are zero, leading to the following system of equations:
	<me>
		\begin{sysofeqns}{rcrcrcr}
			k_1 \amp + \amp k_2 \amp + \amp 2k_3 \amp = \amp 0, \\
			k_1 \amp   \amp     \amp - \amp  k_3 \amp = \amp 0, \\
			    \amp   \amp k_2 \amp + \amp 3k_3 \amp = \amp 0.
		\end{sysofeqns}
	</me>
	Once again, we reduce the coefficient matrix to determine if there are nontrivial solutions:
	<me>
		\left[\begin{array}{rrr} 1 \amp 1 \amp 2 \\ 1 \amp 0 \amp -1 \\ 0 \amp 1 \amp 3 \end{array}\right]
		\qquad\rowredarrow\qquad
		\begin{bmatrix} 1 \amp 0 \amp -1 \\ 0 \amp 1 \amp 3 \\ 0 \amp 0 \amp 0 \end{bmatrix}
	</me>.
	</p>

	<aside><title>Compare</title><p>
		Examine the relationship between the columns of the initial coefficient matrix with the vectors being tested.
	</p></aside>

	<p> Since variable <m>k_3</m> is free, there exist nontrivial solutions and the vectors are <em>linearly dependent</em>. </p>

</example>

<example><title>Testing dependence/independence in <m>F(D)</m></title><p>
	Let's do an example in a function space.
	Consider vectors <m>f(x) = x</m>, <m>g(x) = \sin(\pi x/2)</m> and <m>h(x) = \cos(\pi x/2)</m> in <m>F(\R)</m>,
	the space of functions defined on the whole real number line.
	Are these functions linearly dependent or independent?
	Let's start the test by setting up the vector equation
	<me> k_1 x + k_2\sin(\pi x/2) + k_3\cos(\pi x/2) = 0 </me>.
	Here, there is no algebraic way we can simplify the expression on the left-hand side.
	However, remember that the <m>0</m> on the right-hand side represents the zero <em>function</em>,
	and that functions are only equal when they always produce the same output given the same input (<xref ref="definition-abstract-vec-spaces-examples-eq-funcs" />).
	So let's try substituting some input <m>x</m>-values into the functions on either side of our vector equation above:
	<md><mrow>
		x=0 \amp\colon \amp k_1\cdot 0 + k_2\cdot 0 + k_3 \cdot 1 \amp= 0, \\
		x=1 \amp\colon \amp k_1\cdot 1 + k_2\cdot 1 + k_3 \cdot 0 \amp= 0, \\
		x=2 \amp\colon \amp k_1\cdot 2 + k_2\cdot 0 + k_3 \cdot (-1) \amp= 0.
	</mrow></md>
	From the first equation we see <m>k_3=0</m>.
	Combining this with the third equation we also get <m>k_1=0</m>.
	Then combining that with the second equation we finally get <m>k_2=0</m>.
	Since only the trivial solution is possible, these vectors are <em>linearly independent</em>.
</p></example>

</subsection>


<subsection xml:id="subsection-linear-indep-examples-standard">
<title>Linear independence of <q>standard</q> spanning sets</title>

<p> Finally, let's check the <q>standard</q> spanning sets of our favourite example vector spaces. </p>

<example><title>Independence of the standard basis vectors in <m>\R^n</m></title><p>
	The standard basis vectors <m>\uvec{e}_1,\uvec{e}_2,\dotsc,\uvec{e}_n</m> form a spanning set for <m>\R^n</m>, and they are also linearly independent,
	as we see if we apply the test:
	<md><mrow>
		k_1\uvec{e}_1 + k_2\uvec{e}_1 + \dotsb + k_n\uvec{e}_n \amp= \zerovec \amp
		\amp\implies \amp
		(k_1,k_2,\dotsc,k_n) \amp= (0,0,\dotsc,0)
	</mrow></md>.
	So clearly each scalar <m>k_j</m> must be zero, which means there is only the trivial solution.
</p></example>

<example><title>Independence of the standard spanning vectors in <m>\matrixring_{m \times n}(\R)</m></title><p>
	In <xref ref="remark-subspaces-examples-standard-spanning-sets"/>,
	we noted that there is also a <q>standard</q> set of spanning vectors in <m>\matrixring_{m \times n}(\R)</m>,
	consisting of those matrices that have all zero entries except for a single <m>1</m> in one specific entry.
	We might call these <q>standard basis vectors</q> for <m>\matrixring_{m \times n}(\R)</m>.
	Write <m>E_{ij}</m> for the matrix of this type with a <m>1</m> in the <m>\nth[(i,j)]</m> entry.
	These spanning vectors are also linearly independent.
	Here, when we apply
	the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>,
	it is best if we enumerate our scalars with the same scheme as the vectors:
	<md><mrow>
		k_{11}E_{11} + k_{12}E_{12} + \dotsb + k_{mn}E_{mn} \amp= \zerovec \amp
		\amp\implies \amp
		[k_{ij}] \amp= \zerovec
	</mrow></md>.
	Again, we immediately see that only the trivial solution is possible.
</p></example>

<example><title>Independence of the standard spanning vectors in <m>\poly_n(\R)</m></title><p>
	Also in <xref ref="remark-subspaces-examples-standard-spanning-sets"/>, we noted that the powers of <m>x</m>
	(along with the constant polynomial <m>1</m>)
	form a spanning set for <m>\poly_n(\R)</m>.
	We might call these <q>standard basis vectors</q> for <m>\poly_n(\R)</m>.
	Are they linearly independent?
	Apply the Test:
	<me> k_0 \cdot 1 + k_1x + k_2 x^2 + \dotsb + k_n x^n = 0 </me>.
	If any of these coefficients are nonzero, the polynomial on the left-hand side will be nonzero, so only the trivial solution is possible.
	Therefore, powers of <m>x</m> are always linearly independent in <m>\poly_n(\R)</m>.
</p></example>

</subsection>

</section>
