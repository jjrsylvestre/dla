<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2021 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-col-row-null-space-concepts">

<title>Concepts</title>

<assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-col-row-null-space-concepts-colspace" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-col-row-null-space-concepts-colspace" /></em></li>
<li><xref ref="subsection-col-row-null-space-concepts-row-space" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-col-row-null-space-concepts-row-space" /></em></li>
<li><xref ref="subsection-col-row-null-space-concepts-col-vs-row" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-col-row-null-space-concepts-col-vs-row" /></em></li>
<li><xref ref="subsection-col-row-null-space-concepts-null-space" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-col-row-null-space-concepts-null-space" /></em></li>
</ul></p></assemblage>

<introduction><p>
	We have already seen in
	<xref ref="example-subspaces-examples-solution-space" />
	that the solution set of a homogeneous system
	<m>A\uvec{x} = \zerovec</m> with <m>m\times n</m> coefficient matrix <m>A</m> is a subspace of <m>\R^n</m>.
	We will return to this special subspace at the end of this section,
	but first we will discuss a special subspace of <m>\R^m</m> related to <em>non</em>homogeneous systems with coefficient matrix <m>A</m>.
</p></introduction>


<subsection xml:id="subsection-col-row-null-space-concepts-colspace">
<title>Column space</title>

<paragraphs><title>The <q>consistent space</q> of a coefficient matrix</title>

<p>
The solution set of a nonhomogeneous system
<m>A\uvec{x} = \uvec{b}</m>
with <m>m\times n</m> coefficient matrix <m>A</m> cannot be a subspace of <m>\R^n</m> because it can never contain the zero vector.
Even worse, if the system is inconsistent,
then the solution set does not contain any vectors at all.
</p>

<question><statement><p>
	Amongst <em>all</em> systems with coefficient matrix <m>A</m>, which are consistent?
</p></statement></question>

<p>
We know that the homogeneous system
<m>A\uvec{x} = \zerovec</m>
is consistent because it has at least the trivial solution.
But for what other vectors of <m>\uvec{b}</m> besides <m>\uvec{b} = \zerovec</m> is the system
<m>A\uvec{x}=\uvec{b}</m>
consistent?
<!--
We can show that the collection of all such <m>\uvec{b}</m> vectors is a subspace of <m>\R^m</m> by applying the <xref ref="procedure-subspaces-concepts-test" text="title" />.
<ol label="(i)">
	<li>
		We know this collection is nonempty,
		as we have already noted that
		<m>A\uvec{x}=\uvec{b}</m>
		is consistent for
		<m>\uvec{b}=\zerovec</m>.
	</li>
	<li>
		Suppose system
		<m>A\uvec{x}=\uvec{b}</m>
		is consistent for both
		<m>\uvec{b}=\uvec{b}_1</m>
		and
		<m>\uvec{b}=\uvec{b}_2</m>.
		Then there exists at least one solution to each system.
		Let <m>\uvec{x}=\uvec{x}_1</m> be a solution to
		<m>A\uvec{x}=\uvec{b}_1</m>
		and
		<m>\uvec{x}=\uvec{x}_2</m>
		be a solution to
		<m>A\uvec{x}=\uvec{b}_2</m>.
		Then <m>\uvec{x} = \uvec{x}_1+\uvec{x}_2</m>
		is a solution to system
		<m>A\uvec{x}=\uvec{b}</m>
		for
		<m>\uvec{b}=\uvec{b}_1+\uvec{b}_2</m>,
		since
		<me> A(\uvec{x}_1+\uvec{x}_2) = A\uvec{x}_1+A\uvec{x}_2 = \uvec{b}_1 + \uvec{b}_2. </me>
		Therefore, the system
		<m>A\uvec{x}=\uvec{b}</m>
		is consistent for
		<m>\uvec{b}=\uvec{b}_1+\uvec{b}_2</m>
		because it has at least one solution as above.
		This verifies that whenever vectors
		<m>\uvec{b}_1</m> and <m>\uvec{b}_2</m>
		are in the collection,
		so is the sum vector
		<m>\uvec{b}_1+\uvec{b}_2</m>.
	</li>
	<li>
		Suppose system
		<m>A\uvec{x}=\uvec{b}</m>
		is consistent for
		<m>\uvec{b}=\uvec{b}_0</m>,
		so that there exists at least one solution
		<m>\uvec{x}=\uvec{x}_0</m>.
		Then given a scalar <m>k</m>,
		vector <m>\uvec{x} = k\uvec{x}_0</m>
		is a solution to system
		<m>A\uvec{x}=\uvec{b}</m>
		for
		<m>\uvec{b}=k\uvec{b}_0</m>,
		since
		<me> A(k\uvec{x}_0) = k(A\uvec{x}_0) = k\uvec{b}_0. </me>
		Therefore, the system
		<m>A\uvec{x}=\uvec{b}</m>
		is consistent for
		<m>\uvec{b}=k\uvec{b}_0</m>
		because it has at least one solution as above.
		This verifies that whenever vector <m>\uvec{b}_0</m> is in the collection,
		so is the scaled vector <m>k\uvec{b}_0</m>.
	</li>
</ol>
-->
It is possible to verify directly that the collection of all such <m>\uvec{b}</m> vectors is a subspace of <m>\R^m</m>.
</p>

<aside><title>Check your understanding</title><p>
	Apply the <xref ref="procedure-subspaces-concepts-test" text="title" />
	to verify that for a given <m>m\times n</m> matrix <m>A</m>,
	the collection of all vectors <m>\uvec{b}</m> in <m>\R^m</m> for which the system <m>A\uvec{x}=\uvec{b}</m> is consistent forms a subspace of <m>\R^m</m>.
</p></aside>

<p>
Until we know more about it,
for now let's refer to this subspace of <m>\R^m</m> as the <term>consistent space</term> of <m>A</m>.
</p>

</paragraphs>

<paragraphs><title>Consistent space versus column space</title>

<p>
To better understand this so-called consistent space, we should relate it back to the matrix <m>A</m> as we did in
<xref ref="activity-col-row-null-space-colspan-motivation" />,
because <m>A</m> is the only thing common to all the <m>\uvec{b}</m> vectors in this space.
Let's again think of <m>A</m> as being made up of column vectors in <m>\R^m</m>:
<me>
	A = \begin{bmatrix}
		| \amp | \amp \amp | \\
		\uvec{c}_1 \amp \uvec{c}_2 \amp \cdots \amp \uvec{c}_n \\
		| \amp | \amp  \amp |
	\end{bmatrix}.
</me>
In <xref ref="activity-col-row-null-space-matrix-times-stdvec" />,
we found that the result of computing <m>A\uvec{e}_j</m> is <m>\uvec{c}_j</m>,
the <m>\nth[j]</m> column of <m>A</m>
(where <m>\uvec{e}_j</m> is the <m>\nth[j]</m> standard basis vector in <m>\R^n</m>, as usual).
But this says that each system
<m>A\uvec{x}=\uvec{c}_j</m>
is consistent,
since there is at least one solution <m>\uvec{x}=\uvec{e}_j</m>.
Therefore, each of the columns of <m>A</m> is in the consistent space of <m>A</m>.
And because the span of these columns is the <em>smallest</em> subspace that contains each of them,
we can conclude that every vector in the column space
<m>\Span\{\uvec{c}_1,\uvec{c}_2,\dotsc,\uvec{c}_n\}</m>
of <m>A</m> is also in the consistent space of <m>A</m>.
</p><p>
What other vectors could be in this space?
If <m>A\uvec{x}=\uvec{b}</m> is consistent,
then it has at least one solution
<me>
	\uvec{x}
	= \uvec{x}_0
	= \left[\begin{smallmatrix}x_1\\x_2\\\vdots\\x_n\end{smallmatrix}\right]
	= x_1\uvec{e}_1+x_2\uvec{e}_2+\dotsb+x_n\uvec{e}_n.
</me>
But then
<md>
	<mrow> \uvec{b} \amp = A\uvec{x}_0 </mrow>
	<mrow> \amp = A(x_1\uvec{e}_1+x_2\uvec{e}_2+\dotsb+x_n\uvec{e}_n) </mrow>
	<mrow> \amp = x_1A\uvec{e}_1+x_2A\uvec{e}_2+\dotsb+x_nA\uvec{e}_n </mrow>
	<mrow xml:id="equation-col-row-null-space-concepts-lincomb-columns" tag="star">
		\amp = x_1\uvec{c}_1+x_2\uvec{c}_2+\dotsb+x_n\uvec{c}_n.
	</mrow>
</md>
So whenever <m>A\uvec{x}=\uvec{b}</m> is consistent,
we find that <m>\uvec{b}</m> is equal to some linear combination of the columns of <m>A</m>
(with coefficients taken from the components of a solution vector).
In other words, every vector in the consistent space of <m>A</m> is also in the column space of <m>A</m>.
So the two spaces are equal:
<alert>
	the system <m>A\uvec{x}=\uvec{b}</m> is consistent when,
	and only when,
	the vector of constants <m>\uvec{b}</m> is in the column space of <m>A</m>
</alert>.
</p>

</paragraphs>

<paragraphs><title>Determining a basis for a column space</title>

<p>
Since the columns of <m>A</m> are,
by definition,
a spanning set for the column space of <m>A</m>,
we can reduce it to a basis.
Once again, we can apply row reduction to this task.
Row reducing is equivalent to multiplying on the left by elementary matrices,
and when we defined matrix multiplication in
<xref ref="subsection-matrix-ops-concepts-matrix-mult" />
we did so column-by-column:
<me>
	EA = \begin{bmatrix}
		| \amp | \amp \amp | \\
		E\uvec{c}_1 \amp E\uvec{c}_2 \amp \cdots \amp E\uvec{c}_n \\
		| \amp | \amp  \amp |
	\end{bmatrix}.
</me>
Because matrix multiplication distributes over linear combinations,
<alert>multiplying a collection of column vectors by a common matrix cannot create independence out of dependence</alert>.
Even better, the process of row reducing can be reversed
(<ie /> we are multiplying by <em>invertible</em> matrices),
so it follows that
<alert>multiplying a collection of column vectors by an <em>invertible</em> common matrix cannot create dependence out of independence</alert>.
<aside><title>See</title><p>
	<xref ref="proposition-col-row-null-space-dep-indep-of-images" />
	in
	<xref ref="subsection-col-row-null-space-theory-colspace" />.
</p></aside>
As we partially reasoned in
<xref ref="activity-col-row-null-space-colspace-basis-proc" />,
this means that we can recognize independence/dependence relationships amongst the columns of <m>A</m> from the independence/dependence relationships amongst the columns of reduced forms of <m>A</m>,
leading to the following procedure.
</p>

<algorithm xml:id="procedure-col-row-null-space-concepts-col-space-basis">
	<title>To determine a basis for the column space of matrix <m>A</m></title>
	<statement><p>
	<ol>
		<li> Reduce to at least REF. </li>
		<li> Extract from <m>A</m> all those columns in positions corresponding to the positions of the leading ones in the reduced matrix. </li>
	</ol>
	These extracted columns will form a basis for the column space of <m>A</m>.
	</p></statement>
</algorithm>

<remark><p>
	It is important that you <alert>take the basis vectors from the columns of <m>A</m></alert>,
	not from the columns of the reduced matrix <mdash />
	row operations do not change independence/dependence relationships amongst the columns,
	but they <em>do</em> change the column space.
</p></remark>

</paragraphs>

<paragraphs><title> Using column space to determine linear dependence/independence </title>

<p>
In
<xref ref="activity-col-row-null-space-summarize-col-indep-row-reduce-test-for-indep">Discovery</xref>,
we used this new procedure to create a reinterpretation of
the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>
for vectors in <m>\R^m</m>.
</p>

<algorithm xml:id="procedure-col-row-null-space-concepts-colspace-linear-indep-test">
	<title>To use row reduction to test linear dependence/independence in <m>\R^m</m></title>
	<statement><p>
		To determine whether a collection of <m>n</m> vectors in <m>\R^m</m> is linearly dependent or independent,
		form an <m>m\times n</m> matrix by using the vectors as <em>columns</em>,
		and then row reduce to determine the rank of the matrix.
		If the rank is equal to <m>n</m>
		(<ie /> there is a leading one in every column of the reduced matrix),
		then the vectors are linearly independent.
		If the rank is less that <m>n</m>
		(<ie /> at least one column of the reduced matrix does not contain a leading one),
		then the vectors are linearly dependent.
	</p></statement>
</algorithm>

<p>
Note that this isn't really a new version of
the <xref ref="procedure-linear-indep-concepts-test-for-dep-indep" text="title"/>,
it's just a shortcut <mdash />
if we were to use the full test,
the column vectors we are testing would appear as the columns of the coefficient matrix for the homogeneous system created by the test.
(See
<xref ref="example-linear-indep-examples-test-R4" />,
and the other examples in
<xref ref="subsection-linear-indep-examples-test" />.)
</p>

<p>
In
<xref ref="activity-col-row-null-space-summarize-col-indep-invertible-indep-columns">Discovery</xref>
and
<xref ref="activity-col-row-null-space-summarize-col-indep-det-ne-0-indep-columns">Discovery</xref>,
we also used this new procedure to connect column space to invertibility for a square matrix.
We will summarize these new facts in
<xref ref="subsection-col-row-null-space-theory-spaces-vs-rank-invert" />.
</p>

</paragraphs>

</subsection>


<subsection xml:id="subsection-col-row-null-space-concepts-row-space">
<title>Row space</title>

<p>
Analyzing the row space of a matrix is considerably easier.
As we discovered in
<xref ref="activity-col-row-null-space-row-ops-same-rowspace" />
and
<xref ref="activity-col-row-null-space-summarize-row-space" />,
elementary row operations do not change the row space of a matrix,
so the row spaces of a matrix and each of its REFs are the same space.
Clearly we do not need the zero rows from an REF to span this space.
But the pattern of leading ones guarantees that the nonzero rows in an REF are linearly independent.
</p>
<aside><title>See</title><p>
	<xref ref="corollary-col-row-null-space-rref-row-space" />
	in
	<xref ref="subsection-col-row-null-space-theory-rowspace" />.
</p></aside>

<algorithm>
	<title>To determine a basis for the row space of matrix <m>A</m></title>
	<statement><p>
	<ol>
		<li> Reduce to at least REF. </li>
		<li> Extract the nonzero rows from the REF you have computed. </li>
	</ol>
	These extracted rows will form a basis for the row space of <m>A</m>.
	</p></statement>
</algorithm>

<remark><p>
	Note the difference from the column space procedure <mdash />
	in this procedure we get the basis vectors from the <em>reduced</em> matrix,
	not from the original matrix.
</p></remark>

<p> We can also use the row space procedure to test vectors for linear independence. </p>

<algorithm xml:id="procedure-col-row-null-space-concepts-rowspace-linear-indep-test">
	<title>A second way to use row reduction to test linear dependence/independence in <m>\R^m</m></title>
	<statement><p>
		To determine whether a collection of <m>m</m> vectors in <m>\R^n</m> is linearly dependent or independent,
		form an <m>m\times n</m> matrix by using the vectors as <em>rows</em>,
		and then row reduce to determine the rank of the matrix.
		If the rank is equal to <m>m</m>
		(<ie /> no zero rows can be produced by reducing),
		then the vectors are linearly independent.
		If the rank is less that <m>n</m>
		(<ie /> reducing produces at least one zero row),
		then the vectors are linearly dependent.
	</p></statement>
</algorithm>

</subsection>


<subsection xml:id="subsection-col-row-null-space-concepts-col-vs-row">
<title>Column space versus row space</title>

<question><statement><p>
	Which procedure <mdash /> column space or row space <mdash /> should we use?
</p></statement></question>

<p>
When testing vectors from <m>\R^n</m> for linear independence,
we clearly have a choice of whether to form a matrix using those vectors as columns or as rows.
But we also have a choice when computing a basis for either type of space,
because the column space of a matrix is the same as the row space of the transpose,
and the row space of a matrix is the same as the column space of the transpose.
</p><p>
In <xref ref="activity-col-row-null-space-which-one-col-or-row-space" />,
you were asked to think about this question.
You might have considered the end results of the two procedures to determine the pros and cons of one procedure over the other.
<dl>
	<li><title>Column space</title><p>Produces a basis involving vectors from the original collection.</p></li>
	<li><title>Row space</title><p>Produces a <q>simplified</q> basis.</p></li>
</dl>
In the column space procedure,
we always go back to the original matrix to pick out certain columns.
So, this procedure effectively performs the task of reducing a spanning set down to a basis,
a task that we knew <em>could</em> be done
(<xref ref="proposition-basis-coords-reduce-span-to-basis" />)
but didn't have a systematic means of carrying out.
In the row space procedure,
we take our basis vectors from the simplified nonzero rows of an REF for the matrix.
Because the leading one in each row is in a different position,
expressing other vectors in the space as linear combinations of these basis vectors is much more straightforward than it is in general.
In fact, if you have taken a basis for the row space from the RREF,
expressing other vectors in the space as linear combinations of these basis vectors can be done by inspection.
</p>

</subsection>


<subsection xml:id="subsection-col-row-null-space-concepts-null-space">
<title>Null space and the dimensions of the three spaces</title>

<p>
We have already seen through examples in
<xref ref="subsection-dimension-examples-basis-from-params" />
how to extract a basis for the solution space for a homogeneous system
<m>A\uvec{x}=\zerovec</m>
(now called the <term>null space</term> of <m>A</m>)
from the parameters assigned after row reducing.
</p><p>
The null space of <m>A</m> doesn't just represent the set of solutions to the homogeneous system <mdash />
<xref ref="lemma-matrix-ops-particular-sol-plus-homog-sol" />
tells us that it represents most of the data we need in order to know the solution set of every system that has <m>A</m> as a coefficient matrix.
If we know one specific solution
<m>\uvec{x} = \uvec{x}_1</m>
to nonhomogeneous system
<m>A\uvec{x}=\uvec{b}</m>,
then every other solution can be obtained by adding to <m>\uvec{x}_1</m> a vector from the null space.
Geometrically, this represents a translation of the null space away from the origin,
like a plane that is translated away from the origin by an <q>initial</q> point <m>\uvec{x}_1</m>.
</p><p>
All three spaces
<mdash /> column, row, and null <mdash />
are connected through the RREF of the matrix.
For column space, we get a basis vector for each leading one in the RREF.
For row space, we get a basis vector for each nonzero row in the RREF,
and a row in the RREF is nonzero precisely when it contains a leading one.
So even though column space is a subspace of <m>\R^m</m> and row space is a subspace of <m>\R^n</m>
(where <m>A</m> is an <m>m\times n</m> matrix),
<alert>the column space and the row space of <m>A</m> have the same dimension</alert>,
and this dimension is equal to the rank of <m>A</m>.
On the other hand, for the null space we get one basis vector for each parameter required to solve the homogeneous system.
Parameters are assigned to free variables,
and free variables are those whose columns <em>do not</em> contain a leading one.
So the dimension of the null space is equal to the difference between the number of columns of <m>A</m> and the rank of <m>A</m>,
which is just a more sophisticated way to state
<xref ref="proposition-row-red-num-parameters-vs-rank" />.
</p>

</subsection>

</section>
