<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2024 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-eigen-values-vectors-motivation">

<title>Motivation</title>

<p>
We have seen that when considering a specific matrix <m>A</m>,
looking for patterns in the process of computing matrix-times-column-vector helps us to understand the matrix.
In turn, this helps us understand all of the various systems
<m>A\uvec{x}=\uvec{b}</m>
with common coefficient matrix <m>A</m>,
since obviously the left-hand side of the matrix version of the system has matrix-times-column-vector form.
</p><p>
When we compute <m>A\uvec{e}_j</m> for a standard basis vector <m>\uvec{e}_j</m>,
the result is the <m>\nth[j]</m> column of <m>A</m>.
So if we computed each of
<m>A\uvec{e}_1,A\uvec{e}_2,\dotsc,A\uvec{e}_n</m>,
we would have all of the columns of <m>A</m> as the results,
which contain all of the data contained in <m>A</m>.
These computations certainly let us <em>know</em> the matrix <m>A</m>,
but they don't necessarily help us <em>understand</em> what <m>A</m> is really like as a matrix.
In short, the standard basis for <m>\R^n</m> is a great basis for understanding the vector space <m>\R^n</m>,
but it is not so great for helping understand matrix products <m>A\uvec{x}</m> for a particular matrix <m>A</m>.
</p><p>
In
<xref ref="activity-eigen-values-vectors-eigen-motivation" />,
we discovered that for an <m>n\times n</m> matrix <m>A</m>,
if we can build a basis for <m>\R^n</m> consisting of eigenvectors of <m>A</m>,
then every matrix product <m>A\uvec{x}</m> becomes simple to compute once <m>\uvec{x}</m> is decomposed as a linear combination of these basis vectors.
Indeed, if
<m> \{\uvec{u}_1,\uvec{u}_2,\dotsc,\uvec{u}_n\} </m>
is a basis for <m>\R^n</m>,
and we have
<md><mrow>
	A\uvec{u}_1 \amp= \lambda_1\uvec{u}_1, \amp
	A\uvec{u}_2 \amp= \lambda_2\uvec{u}_2, \amp
	\amp\dotsc, \amp
	A\uvec{u}_n \amp= \lambda_n\uvec{u}_n,
</mrow></md>
then multiplication by <m>A</m> can be achieved by scalar multiplication:
<md>
	<mrow>
		\amp\amp \uvec{x} \amp= k_1\uvec{u}_1 + k_2\uvec{u}_2 + \dotsb + k_n\uvec{u}_n
	</mrow><mrow></mrow><mrow>
		\amp\implies \amp
		A\uvec{x} \amp= k_1A\uvec{u}_1 + k_2A\uvec{u}_2 + \dotsb + k_nA\uvec{u}_n
	</mrow><mrow>
		\amp\amp\amp= k_1\lambda_1\uvec{u}_1 + k_2\lambda_2\uvec{u}_2 + \dotsb + k_n\lambda_n\uvec{u}_n.
	</mrow>
</md>
</p><p>
<!-- TODO rewrite if matrix forms part is extended -->
A complete study of how the concepts of eigenvalues and eigenvectors unlock all the mysteries of a matrix is too involved to carry out in full at this point,
but we will get a glimpse of how it all works for a certain kind of square matrix in the next chapter.
For the remainder of this chapter,
we will be more concerned with how to calculate eigenvalues and eigenvectors.
</p>

</section>
