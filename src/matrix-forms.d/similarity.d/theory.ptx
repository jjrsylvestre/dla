<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2024 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-similarity-theory">

<title>Theory</title>

<introduction><assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-similarity-theory-equiv-rel" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-similarity-theory-equiv-rel" /></em></li>
<li><xref ref="subsection-similarity-theory-props" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-similarity-theory-props" /></em></li>
</ul></p></assemblage></introduction>

<subsection xml:id="subsection-similarity-theory-equiv-rel">
<title>Similarity is an equivalence relation</title>

<p>
We will record here the result of
<xref ref="activity-similarity-equiv-rel" />
without proof,
as we have already justified it in that activity.
</p>

<theorem xml:id="theorem-similarity-equiv-rel"><statement><p>
	Similarity is an equivalence relation on square matrices of a particular size.
	That is, similarity is reflexive, symmetric, and transitive.
</p></statement></theorem>

<p>
As discussed in
<xref ref="subsection-similarity-concepts-classes" />,
<xref ref="theorem-similarity-equiv-rel" />
implies that similarity partitions <m>\matrixring_n(\C)</m> into <term>similarity classes</term>.
There are some special similarity classes,
which explains our initial focus on diagonal form
(<xref ref="chapter-diagonalization" />)
as the first <q>simple</q> form of interest.
</p>

<proposition xml:id="proposition-similarity-scalar-sim-classes">
	<title>Scalar similarity classes</title>
	<statement><p> The similarity class of a scalar matrix contains only that matrix. </p></statement>
	<proof><title>Proof idea</title><p>
		All that is required is to prove that a scalar matrix <m>k I</m> can <em>only</em> be similar to itself.
		That is, if <m>B \similar k I</m> is true, then it must in fact be that <m>B = kI</m>.
		We leave the details to you, the reader.
	</p></proof>
</proposition>

<remark><p> Note that this proposition applies to both the zero matrix <m>\zerovec = 0 I</m> and the identity matrix <m>I = 1 I</m>. </p></remark>

</subsection>

<subsection xml:id="subsection-similarity-theory-props">
<title>Properties of similar matrices</title>

<p> Now we'll state and prove the properties of similar matrices that can be transferred from one to the other via a transition matrix that realizes the similarity. </p>

<proposition xml:id="proposition-similarity-powers">
	<title>Powers of similar are similar</title>
	<statement><p>
		If <m>A</m> and <m>B</m> are similar,
		then so are <m>A^k</m> and <m>B^k</m> for every positive integer <m>k</m>.
		If, in addition, we assume that one of <m>A,B</m> is invertible,
		then the other must be as well,
		and we may expand our first claim to include <em>every</em> integer <m>k</m>.
	</p></statement>
	<proof>
		<p> Assume <m>B = \inv{P} A P</m>. </p>
		<case><title>Case of positive <m>k</m></title><p>
			We have
			<me> B^k = (\inv{P} A P)^k = (\inv{P} A P) (\inv{P} A P) (\inv{P} A P) \dotsm (\inv{P} A P), </me>
			where the expression on the right has <m>k</m> factors of <m>(\inv{P} A P)</m>.
			By associativity, rearrange the brackets:
			<me> B^k = \inv{P} A (P \inv{P}) A (P \inv{P}) A (P \dotsm \inv{P}) A P </me>.
			Each factor of <m>P \inv{P}</m> is equal to the identity matrix,
			and the identity matrix has no effect in matrix multiplication.
			Thus,
			<me> B^k = \inv{P} A A A \dotsm A P, </me>
			where there are <m>k</m> factors of <m>A</m>,
			since we started with <m>k</m> factors of <m>(\inv{P} A P)</m>.
			We now have <m>B^k = \inv{P} A^k P</m>,
			showing that <m>A^k</m> and <m>B^k</m> are similar, as desired.
		</p></case>
		<p>
		If we assume that <m>A</m> is invertible,
		then so is <m>B</m> since it is equal to the product of invertible matrices <m>\inv{P}</m>, <m>A</m>, and <m>P</m>
		(<xref ref="proposition-inverses-props-of-inverses-inverse-extended-product">Statement</xref>
		of
		<xref ref="proposition-inverses-props-of-inverses" />).
		If we instead assume that <m>B</m> is invertible,
		then we can make the same argument using expression <m>A = P B \inv{P}</m> to conclude that <m>A</m> is invertible as well.
		</p><p>
		For the remainder of the cases of showing similarity of powers of <m>A</m> and <m>B</m>,
		assume that <m>A</m> and <m>B</m> are both invertible.
		</p>
		<case><title>Case of <m>k = 0</m></title><p>
			By convention, both <m>A^0 = I</m> and <m>B^0 = I</m>,
			and the identity matrix is similar to itself.
		</p></case>
		<case><title>Case of <m>k = -1</m></title><p>
			Demonstrating similarity of <m>\inv{A}</m> and <m>\inv{B}</m> is straightforward,
			and we leave it to you, the reader.
		</p></case>
		<case><title>Case of <m>k <lt /> -1</m></title><p>
			Write <m>k = - m</m> for <m>m</m> a positive integer.
			If <m>A</m> and <m>B</m> are similar,
			then so are <m>\inv{A}</m> and <m>\inv{B}</m> by a previous case.
			But then so are positive powers of <m>\inv{A}</m> and <m>\inv{B}</m>,
			also by a previous case.
			In particular, the matrices
			<md><mrow>
				\bigl(\inv{A}\bigr)^m \amp = A^{-m} = A^k \text{,} \amp
				\bigl(\inv{B}\bigr)^m \amp = B^{-m} = B^k
			</mrow></md>
			are similar.
		</p></case>
	</proof>
</proposition>

<proposition xml:id="proposition-similarity-null-col-spaces">
	<statement><p>
		Assume <m>B = \inv{P} A P</m>.
		Then
		<ol>
			<li xml:id="proposition-similarity-null-col-spaces-null">
				The transition matrix <m>P</m> transforms null space vectors of <m>B</m> into null space vectors of <m>A</m>.
			</li>
			<li xml:id="proposition-similarity-null-col-spaces-col">
				The transition matrix <m>P</m> transforms column space vectors of <m>B</m> into column space vectors of <m>A</m>.
			</li>
		</ol>
	</p></statement>
	<proof>
		<title>Proof idea for <xref ref="proposition-similarity-null-col-spaces-null">Statement</xref></title>
		<p>
		Assume <m>\uvec{x} = \uvec{x}_0</m> is a solution to <m>B \uvec{x} = \zerovec</m>,
		so that <m>B \uvec{x}_0 = \zerovec</m> is true.
		Substitute the similarity relation <m>B = \inv{P} A P</m> into this vector equality and rearrange to demonstrate that <m>\uvec{x} = P \uvec{x}_0</m> is a solution to
		<m>A \uvec{x} = \zerovec</m>.
		</p>
	</proof>
	<proof>
		<title>Proof idea for <xref ref="proposition-similarity-null-col-spaces-col">Statement</xref></title>
		<p>
		If vector <m>\uvec{b}</m> is in the column space of <m>B</m>, then the system <m>B \uvec{x} = \uvec{b}</m> is consistent.
		(See the discussion in <xref ref="subsection-col-row-null-space-concepts-colspace"/>.)
		So suppose <m>\uvec{x} = \uvec{x}_0</m> is a solution to this system,
		so that <m>B \uvec{x}_0 = \uvec{b}</m> is true.
		Substitute the similarity relation <m>B = \inv{P} A P</m> into this vector equality and rearrange to demonstrate that
		<m>\uvec{x} = P \uvec{x}_0</m> is a solution to the system <m>A \uvec{x} = P \uvec{b}</m>.
		Hence this system is consistent,
		which means that <m>P \uvec{b}</m> is in the column space of <m>A</m>.
		</p>
	</proof>
</proposition>

<p> The following corollary follows from the proposition due to the fact that the transition matrix <m>P</m> is invertible. </p>

<corollary xml:id="corollary-similarity-rank-nullity">
	<title>Rank and nullity of similar matrices</title>
	<statement><p>
		If <m>A</m> and <m>B</m> are similar matrices then the dimensions of the null spaces of the two matrices are equal,
		and also the dimensions of the column spaces of the two matrices are equal.
		In other words, similar matrices have the same nullity and rank.
	</p></statement>
	<proof><title>Proof idea</title>
		<p>
		We will outline the proof of equal nullities by considering maximally linearly independent sets of null space vectors;
		the proof of equal ranks can be similarly obtained by considering maximally linearly sets of column space vectors.
		</p>
		<p>
		We have already seen in <xref ref="proposition-similarity-null-col-spaces" /> that if <m>B = \inv{P} A P</m>,
		then the transition matrix <m>P</m> transforms null space vectors of <m>B</m> into null space vectors of <m>A</m>.
		The symmetry of the similarity relation tells that <m>\inv{P}</m> will do the reverse:
		transform null space vectors of <m>A</m> into null space vectors of <m>B</m>.
		Applying <xref ref="proposition-col-row-null-space-dep-indep-of-images-indep-implies-indep-of-images">Statement</xref>
		of <xref ref="proposition-col-row-null-space-dep-indep-of-images" />
		with <m>E = P</m> to a maximal linearly independent set of null space vectors of <m>B</m>,
		or with <m>E = \inv{P}</m> to a maximal linearly independent set of null space vectors of <m>A</m>,
		we see that such maximal linearly independent sets must be the same size for <m>A</m> as for <m>B</m>.
		As dimension is the size of a basis, and a basis is a maximally linearly independent set
		(see <xref ref="theorem-basis-coords-basis-is-minimal-maximal-basis-is-maximal" text="type-local">Statement</xref>
		of <xref ref="theorem-basis-coords-basis-is-minimal-maximal" />)
		the dimensions of the two null spaces must be the same.
	</p></proof>
</corollary>

<p>
Now we record some facts relating eigenvalues and eigenvectors between similar matrices.
Note that we have already recorded (and proved) some of these facts in <xref ref="subsection-diagonalization-theory-similar" />.
</p>

<proposition xml:id="proposition-similarity-similar-same-det">
	<!-- TODO add same trace? -->
	<statement><p> Similar matrices have the same determinant. </p></statement>
	<proof><p>
		We have already stated and proved this fact as <xref ref="proposition-diagonalization-similar-matrices-properties-same-det">Statement</xref>
		of <xref ref="proposition-diagonalization-similar-matrices-properties" />.
	</p></proof>
</proposition>

<theorem xml:id="theorem-similarity-similar-same-char-poly-evals-transformed-evecs">
	<statement><p>
		Similar matrices have the same characteristic polynomial,
		hence also the same eigenvalues with the same algebraic multiplicities.
		Moreover,
		if <m>B = \inv{P} A P</m>,
		then for each shared eigenvalue the transition matrix <m>P</m> transforms eigenvectors of <m>B</m> into eigenvectors of <m>A</m>.
	</p></statement>
	<proof><title>Proof idea</title>
		<p>
		Note that we have already stated and proved the fact about eigenvalues and their multiplicities as
		<xref ref="proposition-diagonalization-similar-matrices-properties-same-evalues">Statement</xref>
		of <xref ref="proposition-diagonalization-similar-matrices-properties" />.
		So all that remains is to prove the proposed fact about eigenvectors.
		</p>
		<p>
		So assume <m>B = \inv{P} A P</m>,
		and suppose <m>\uvec{x}_0</m> is an eigenvector of <m>B</m> corresponding to eigenvalue <m>\lambda</m>.
		By definition, this means <me>B \uvec{x}_0 = \lambda \uvec{x}_0</me>.
		Substitute the assumed similarity relation for <m>A</m> and <m>B</m> and rearrange to <me>A (P \uvec{x}_0) = \lambda (P \uvec{x}_0)</me>,
		which says that <m>P \uvec{x}_0</m> is an eigenvector of <m>A</m> corresponding to the shared eigenvalue <m>\lambda</m>.
	</p></proof>
</theorem>

<p>
Similarly to null spaces and column spaces of similar matrices above,
the following corollary follows from the theorem due to the fact that the transition matrix <m>P</m> is invertible.
</p>

<corollary>
	<statement><p>
		If <m>A</m> and <m>B</m> are similar matrices and <m>\lambda</m> is a shared eigenvalue of these two matrices,
		then the eigenspace of <m>A</m> corresponding to <m>\lambda</m> has the same dimension as the eigenspace of <m>B</m> corresponding to <m>\lambda</m>.
		In other words,
		the geometric multiplicity of <m>\lambda</m> as an eigenvalue of <m>A</m> is the same as the geometric multiplicity of <m>\lambda</m> as an eigenvalue of <m>B</m>.
	</p></statement>
	<proof><title>Proof idea</title><p>
		This is essentially the same as the proof of <xref ref="corollary-similarity-rank-nullity" />:
		if <m>B = \inv{P} A P</m>,
		then from <xref ref="theorem-similarity-similar-same-char-poly-evals-transformed-evecs" /> we know that the transition matrix <m>P</m> will transform vectors in the eigenspace <m>E_\lambda(B)</m> into vectors in the eigenspace <m>E_\lambda(A)</m> for any shared eigenvalue of <m>A</m> and <m>B</m>,
		and <m>\inv{P}</m> will do the reverse.
		Applying <xref ref="proposition-col-row-null-space-dep-indep-of-images-indep-implies-indep-of-images">Statement</xref>
		of <xref ref="proposition-col-row-null-space-dep-indep-of-images" />
		with <m>E = P</m> to a maximal linearly independent set of vectors in <m>E_\lambda(B)</m>,
		or with <m>E = \inv{P}</m> to a maximal linearly independent set of vectors in <m>E_\lambda(A)</m>,
		we see that such maximal linearly independent sets must be the same size for <m>A</m> as for <m>B</m>.
	</p></proof>
</corollary>

</subsection>

</section>
