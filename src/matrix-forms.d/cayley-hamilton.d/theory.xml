<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2021 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-cayley-hamilton-theory">

<title>Theory</title>

<assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-cayley-hamilton-theory-nilpotent" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-cayley-hamilton-theory-nilpotent" /></em></li>
<li><xref ref="subsection-cayley-hamilton-theory-theorem" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-cayley-hamilton-theory-theorem" /></em></li>
<li><xref ref="subsection-cayley-hamilton-theory-more" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-cayley-hamilton-theory-more" /></em></li>
</ul></p></assemblage>

<subsection xml:id="subsection-cayley-hamilton-theory-nilpotent">
<title>Nilpotent matrices</title>

<p> First, we establish the pattern we've seen so many times in this chapter already. </p>

<lemma xml:id="lemma-cayley-hamilton-scalar-triang-nilpotent">
	<title>Scalar-triangular with <m>\lambda = 0</m> is nilpotent</title>

	<statement><p>
		A scalar-triangular matrix <m>A</m> with <m>\lambda = 0</m> down the main diagonal will satisfy <m>A^n = \zerovec</m>,
		where <m>n</m> is the size of <m>A</m>.
		In particular, such a matrix is always nilpotent.
	</p></statement>

	<proof><title>Proof outline</title>

		<p>
		We omit a detailed proof,
		relying on <xref ref="example-cayley-hamilton-scalar-triang-nilpotent" /> to demonstrate the pattern.
		</p>

		<p>
		However, a proof by induction is fairly straightforward,
		as a matrix of this form can be broken up into a block matrix
		<me> A = \left[\begin{array}{@{}c|c@{}} 0 \amp R \\ \hline \zerovec \amp B \end{array}\right] </me>,
		where <m>B</m> is an <m>(n-1) \times (n-1)</m> matrix of the same form,
		<m>R</m> is a <m>1 \times (n-1)</m> row vector,
		and <m>\zerovec</m> represents an <m>(n-1) \times 1</m> zero vector.
		Using the pattern of block multiplication from <xref ref="subsection-block-diag-concepts-block-ops" />,
		one can calculate
		<me> A^k = \left[\begin{array}{@{}c|c@{}} 0 \amp RB^{k-1} \\ \hline \zerovec \amp B^k \end{array}\right] </me>
		for any positive exponent <m>k</m>,
		and from there the result follows readily using the appropriate induction hypothesis.
		</p>

	</proof>

</lemma>

<p>
Despite the lemma above,
keep <xref ref="example-cayley-hamilton-non-triang-nilpotent" /> in mind here <mdash />
a matrix does not have to be triangular to be nilpotent.
</p>

<p>
The next proposition should not be a surprise,
since we know that similar matrices are effectively the same matrix,
just <q>expressed</q> in different coordinate systems.
</p>

<proposition xml:id="proposition-cayley-hamilton-similar-to-nilpotent">
	<title>Similar to nilpotent is nilpotent of same degree</title>

	<statement><p>
		A matrix that is similar to a nilpotent one must itself also be nilpotent,
		and the two similar matrices must have the same <xref ref="definition-cayley-hamilton-degree-nilpotency" text="title" />.
	</p></statement>

	<proof><title>Outline of proof</title><p>
		Suppose we have a pair of similar matrices.
		By <xref ref="proposition-similarity-powers" />,
		these two matrices also have similar powers.
		Since the only matrix that is similar to the zero matrix is the zero matrix itself,
		if a power of one of the two matrices becomes zero at a certain power,
		then so will the other.
	</p></proof>

</proposition>

<p>
Now we will record some characterizations of nilpotency.
As each of these statements is equivalent to nipotency,
we can use these statements both as properties of nilpotent matrices and as ways to recognize that a matrix is nilpotent.
</p>

<theorem xml:id="theorem-cayley-hamilton-characterizations-of-nilpotency">
	<title>Characterizations of nilpotency</title>

	<statement><p>
		For a square matrix <m>A</m>,
		the following are equivalent.
		<ol>

			<li xml:id="theorem-cayley-hamilton-characterizations-of-nilpotency-nilpotent">
				Matrix <m>A</m> is nilpotent.
			</li>

			<li xml:id="theorem-cayley-hamilton-characterizations-of-nilpotency-evalues">
				<title>Eigenvalues and characteristic polynomial</title>
				<p>
				Matrix <m>A</m> has only a single eigenvalue <m>\lambda = 0</m>,
				and its characteristic polynomial must be <m>c_A(\lambda) = \lambda^n</m>.
				</p>
			</li>

			<li xml:id="theorem-cayley-hamilton-characterizations-of-nilpotency-triang-block">
				<title>Triangular block form</title>
				<p> Matrix <m>A</m> is similar to a scalar-triangular matrix with <m>\lambda = 0</m> down the main diagonal. </p>
			</li>

			<li xml:id="theorem-cayley-hamilton-characterizations-of-nilpotency-max-nilpotency">
				<title>Maximum degree of nilpotency</title>
				<p> Matrix <m>A</m> satisfies <m>A^n = \zerovec</m>. </p>
			</li>

		</ol>
	</p></statement>

	<proof>
		<p> We will show that each statement of the theorem implies the next. </p>

		<case>
			<title>
				Whenever
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-nilpotent">Statement</xref>
				is true,
				then so is
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-evalues">Statement</xref>
			</title>

			<p>
			Suppose <m>A</m> is a square matrix,
			<m>\lambda = \lambda_0</m> is an eigenvalue of <m>A</m>,
			and <m>\uvec{x}</m> is a (nonzero) eigenvector of <m>A</m> corresponding to <m>\lambda_0</m>.
			For every positive exponent <m>k</m>, we have
			<me> A^k \uvec{x} = \lambda_0^k \uvec{x} </me>.
			But if <m>A</m> is nilpotent with <m>A^k = \zerovec</m> for some <m>k</m>,
			then the above becomes
			<me> \zerovec = \lambda_0^k \uvec{x} </me>.
			Since <m>\uvec{x}</m> is assumed nonzero,
			we must have <m>\lambda_0^k = 0</m>
			(<xref ref="proposition-abstract-vec-spaces-basic-vec-props-zero-scalar-mul-eq">Rule</xref>
			of
			<xref ref="proposition-abstract-vec-spaces-basic-vec-props" />),
			which forces <m>\lambda_0 = 0</m> as well.
			</p>

			<p>
			We have established that when <m>A</m> is nilpotent,
			it cannot have any eigenvalues, real or complex, besides <m>\lambda = 0</m>.
			The assertion that <m>c_A(\lambda) = \lambda^n</m> immediately follows.
			</p>

		</case>

		<case>
			<title>
				Whenever
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-evalues">Statement</xref>
				is true,
				then so is
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-triang-block">Statement</xref>
			</title>
			<p> This is <xref ref="corollary-triang-block-scalar-triang" /> with <m>\lambda = 0</m>. </p>
		</case>

		<case>
			<title>
				Whenever
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-triang-block">Statement</xref>
				is true,
				then so is
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-max-nilpotency">Statement</xref>
			</title>
			<p>
			Suppose there exists invertible <m>P</m> so that <m>\inv{P} A P = N</m>,
			where <m>N</m> is scalar-triangular matrix with <m>\lambda = 0</m> down the main diagonal.
			<xref ref="lemma-cayley-hamilton-scalar-triang-nilpotent" /> tells that such a matrix <m>N</m> will satisfy <m>N^n = \zerovec</m>.
			Combined with the identity <me> {(\inv{P} A P)}^n = \inv{P} A^n P </me>,
			we may conclude that <m>A^n = \zerovec</m> as well,
			since the only matrix that is similar to the zero matrix is the zero matrix itself.
			</p>
		</case>

		<case>
			<title>
				Whenever
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-max-nilpotency">Statement</xref>
				is true,
				then so is
				<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-nilpotent">Statement</xref>
			</title>
			<p>
			If we assume <m>A</m> satisfies <m>A^n = \zerovec</m>,
			then by definition we are assuming that <m>A</m> is, in fact, a <term><xref ref="definition-cayley-hamilton-nilpotent" text="title" /></term>.
			</p>
		</case>

	</proof>

</theorem>

<remark><p><ol>

	<li>
		As we have seen in <xref ref="example-cayley-hamilton-non-triang-nilpotent" />,
		a matrix does not have to be scalar-triangular with zeros down its diagonal in order to be nilpotent.
		But <xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-triang-block">Statement</xref>
		of <xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency" />
		does say that a matrix has to at least be similar to such a scalar-triangular matrix.
	</li>

	<li>
		<xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-max-nilpotency">Statement</xref>
		of <xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency" />
		does not say that every nilpotent <m>n \times n</m> matrix
		has <xref ref="definition-cayley-hamilton-degree-nilpotency" text="title" /> equal to <m>n</m>.
		Instead, it tells us that degree of nilpotency will be <em>no greater</em> than <m>n</m>.
	</li>

</ol></p></remark>

<p>
Finally,
a matrix whose powers eventually become zero does not sound like it could be invertible,
and that is the case.
</p>

<corollary><title>Nilpotent is singular</title>

	<statement><p> A nilpotent matrix is always singular. </p></statement>

	<proof><p>
		Combine <xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency-evalues">Statement</xref>
		of <xref ref="theorem-cayley-hamilton-characterizations-of-nilpotency" />
		with the equivalence of <xref ref="theorem-eigen-values-vectors-equiv-to-invertible-invertible">Statement</xref>
		and <xref ref="theorem-eigen-values-vectors-equiv-to-invertible-evalue-0">Statement</xref>
		in <xref ref="theorem-eigen-values-vectors-equiv-to-invertible" />.
	</p></proof>

</corollary>

</subsection>

<subsection xml:id="subsection-cayley-hamilton-theory-theorem">
<title>The Cayley-Hamilton Theorem</title>

<p>
First,
we establish the similarity pattern of matrix polynomials explored in
<xref ref="activity-cayley-hamilton-matrix-poly-similarity" />
and discussed in
<xref ref="subsection-cayley-hamilton-concepts-matrix-poly" />.
</p>

<lemma xml:id="lemma-cayley-hamilton-poly-of-similar">
	<title>Similar matrices evaluate to similar results</title>

	<statement><p>
		Suppose <m>p(x)</m> is a polynomial and <m>A</m> and <m>P</m> are square matrices of the same size with <m>P</m> invertible.
		Then, <me> p(\inv{P} A P) = \inv{P} \bigl(p(A)\bigr) P </me>.
	</p></statement>

	<proof><title>Proof outline</title><p>
		The case that <m>p(x)</m> consists of just a power of <m>x</m>,
		so that <m>p(x) = x^k</m>,
		is proved exactly as in the proof of
		<xref ref="proposition-similarity-powers" />.
		And then the case of a general polynomial
		<me> p(x) = a_0 + a_1 x + a_2 x^2 + \dotsb + a_m x^m </me>
		follows from single-power case,
		and is left as an exercise for you, the reader.
	</p></proof>

</lemma>

<p> Now we'll record and prove the main result of this chapter. </p>

<theorem xml:id="theorem-cayley-hamilton">
	<idx><h>Cayley-Hamilton Theorem</h></idx>
	<idx><h>Theorem</h><h>Cayley-Hamilton</h></idx>
	<title>The Cayley-Hamilton Theorem</title>

	<statement><p> For every square matrix <m>A</m>, <m>c_A(A) = \zerovec</m>. </p></statement>

	<proof>

		<p>
		As in <xref ref="activity-cayley-hamilton-triang-block-case" />,
		first we will prove the statement in the case that <m>A</m> is a matrix in triangular block form.
		The eigenvalues of such a matrix are precisely the diagonal entries,
		and each algebraic multiplicity is equal to the size of the corresponding scalar-triangular block along the block diagonal.
		So the characteristic polynomial of <m>A</m> will be of the form
		<md><mrow xml:id="equation-cayley-hamilton-fully-factored-char-poly" tag="star">
			c_A(\lambda) = (\lambda - \lambda_1)^{m_1} (\lambda - \lambda_2)^{m_2} \dotsm (\lambda - \lambda_\ell)^{m_\ell}
		</mrow></md>,
		where we assume that the linear factors appear in the same order according to eigenvalue that the scalar-triangular blocks appear down the block diagonal of <m>A</m>.
		Evaluating <m>c_A(\lambda)</m> at <m>A</m> gives
		<me> c_A(A) = (A - \lambda_1 I)^{m_1} (A - \lambda_2 I)^{m_2} \dotsm (A - \lambda_\ell I)^{m_\ell} </me>.
		Now, each linear expression
		<me> A - \lambda_j I </me>
		will also be in triangular block form,
		but the blocks will now have the <em>differences</em> of eigenvalues against the specific eigenvalue <m>\lambda_j</m> down their main diagonals.
		In particular, the <m>\nth[j]</m> scalar-triangular block will have zero repeated down the main diagonal.
		And since that <m>\nth[j]</m> block has size equal to the corresponding algebraic multiplicity <m>m_j</m>,
		raising it to the exponent <m>m_j</m> will result in the zero matrix
		(<xref ref="lemma-cayley-hamilton-scalar-triang-nilpotent" />).
		Applying <xref ref="proposition-block-diag-props-products-powers">Statement</xref>
		of <xref ref="proposition-block-diag-props" />,
		each factor
		<me> (A - \lambda_j I)^{m_j} </me>
		will have a zero block,
		and these will appear in descending positions down the block diagonal from factor to factor as <m>j</m> increases from <m>1</m> to <m>\ell</m>.
		Multiplying these factors together is achieved by multiplying together all the corresponding block matrices from the block diagonals,
		and this will result in zero for each block position since each such product will have a zero factor in it.
		Therefore, <m>c_A(A) = \zerovec</m>.
		</p>

		<p>
		Now we proceed as in <xref ref="activity-cayley-hamilton-triang-general-case" />.
		Suppose that <m>A</m> is a general square matrix,
		not necessarily in any special form.
		Even if <m>A</m> is a real matrix,
		we will consider it to be a complex matrix (every real number is also a complex number!),
		and we will work over the field of scalars <m>\C</m>.
		<xref ref="theorem-complex-fund-thm-alg-complex" text="title" /> implies that every polynomial,
		considered as a complex polynomial,
		factors completely into linear factors.
		(See <xref ref="corollary-complex-fund-thm-alg-complex-factoring" />.)
		Therefore, the characteristic polynomial of <m>A</m> can be factored completely as in
		<xref ref="equation-cayley-hamilton-fully-factored-char-poly" /> above,
		and so <xref ref="theorem-triang-block-form" />
		can be applied to obtain a matrix <m>T</m> in triangular block form to which <m>A</m> is similar.
		From the first case above, we know <me> c_T(T) = \zerovec </me>.
		But similar matrices have the same characteristic polynomial
		(<xref ref="theorem-similarity-similar-same-char-poly-evals-transformed-evecs" />)
		and evaluate to similar results in polynomials
		(<xref ref="lemma-cayley-hamilton-poly-of-similar" />),
		so <m> c_A(A) </m> is similar to <m> c_T(T) = \zerovec </m>.
		However, the only matrix that is similar to the zero matrix is the zero matrix itself,
		so we can conclude that <me> c_A(A) = \zerovec </me>.
		</p>

	</proof>

</theorem>

</subsection>

<subsection xml:id="subsection-cayley-hamilton-theory-more">
<title>More consequences of triangular block form</title>

<p>
Finally,
we'll record the patterns we discovered in
<xref ref="activity-cayley-hamilton-det-trace-vs-evals-charpoly" />.
</p>

<theorem xml:id="theorem-cayley-hamilton-det-trace-vs-char-poly-evals">
	<title>Determinant and trace versus characteristic polynomial and eigenvalues</title>

	<statement><p>
		Suppose <m>A</m> is an <m>n \times n</m> matrix.
		<ol>

			<li><p><ol>

				<li>
					The determinant of <m>A</m> is equal to the product of its eigenvalues, both real and complex,
					and with each eigenvalue repeated in the product according to its algebraic multiplicity.
				</li>

				<li> The constant term in the characteristic polynomial of <m>A</m> is always <m>(-1)^n \det A</m>. </li>

			</ol></p></li>

			<li><p><ol>

				<li>
					The trace of <m>A</m> is equal to the sum of its eigenvalues, both real and complex,
					and with each eigenvalue repeated in the sum according to its algebraic multiplicity.
				</li>

				<li> The <m>\lambda^{n-1}</m> term in the characteristic polynomial of <m>A</m> always has coefficient <m>- \trace A</m>. </li>

			</ol></p></li>

		</ol>
	</p></statement>

	<proof>

		<p>
		Since similar matrices have the same determinant, trace, characteristic polynomial, and eigenvalues,
		and since every matrix is similar (over <m>\C</m>) to a matrix in triangular block form,
		it suffices to consider the case of triangular block form.
		So suppose <m>A</m> is a triangular block matrix,
		and its characteristic polynomial factors as a complex polynomial into
		<md><mrow xml:id="equation-cayley-hamilton-fully-factored-char-poly2" tag="dstar">
			c_A(\lambda) = (\lambda - \lambda_1)^{m_1} (\lambda - \lambda_2)^{m_2} \dotsm (\lambda - \lambda_\ell)^{m_\ell}
		</mrow></md>.
		</p>

		<p>
		The <m>\nth[j]</m> block of <m>A</m> is scalar-triangular with determinant equal to <m>\lambda_j^{m_j}</m>,
		the product of its diagonal entries.
		Applying <xref ref="proposition-block-diag-props-det">Statement</xref>
		of <xref ref="proposition-block-diag-props" />,
		we immediately get
		<me> \det A = \lambda_1^{m_1} \lambda_2^{m_2} \dotsm \lambda_\ell^{m_\ell} </me>.
		Determining the constant term of <m>c_A(\lambda)</m> is as easy as evaluating the polynomial at <m>\lambda = 0</m>,
		which yields
		<md>
			<mrow> c_A(0) \amp = (- \lambda_1)^{m_1} (- \lambda_2)^{m_2} \dotsm (- \lambda_\ell)^{m_\ell} </mrow>
			<mrow> \amp = (-1)^{m_1 + m_2 + \dotsb + m_\ell} \lambda_1^{m_1} \lambda_2^{m_2} \dotsm \lambda_\ell^{m_\ell} </mrow>
			<mrow> \amp = (-1)^n \det A </mrow>
		</md>,
		where
		<me> m_1 + m_2 + \dotsb + m_\ell = n </me>
		because the degree of a characteristic polynomial is always equal to the size of the matrix.
		</p>

		<p>
		Similarly, the trace of the <m>\nth[j]</m> block of <m>A</m> is equal to <m>m_j \lambda_j</m>,
		the sum of its diagonal entries.
		The trace of <m>A</m> is then clearly equal to the sum of these sums of diagonal entries, so that
		<me> \trace A = m_1 \lambda_1 + m_2 \lambda_2 + \dotsb + m_\ell \lambda_\ell </me>.
		To investigate the coefficient on <m>\lambda^{n-1}</m> in
		<xref ref="equation-cayley-hamilton-fully-factored-char-poly2" />,
		first consider this expression as
		<me> c_A(\lambda) = \bigl[(\lambda - \lambda_1) \dotsm (\lambda - \lambda_1)\bigr] \dotsm \bigl[(\lambda - \lambda_\ell) \dotsm (\lambda - \lambda_\ell)\bigr] </me>,
		where each factor is repeated according to algebraic multiplicity.
		Similar to how the Binomial Theorem is investigated,
		in expanding the above expression we get a term involving <m>\lambda^{n-1}</m> by choosing <m>\lambda</m> from all but one of the linear factors and choosing <m>(-\lambda_j)</m> from the remaining factor,
		and multiplying all these choices together.
		Each such product will be equal to <m>-\lambda_j \lambda^{n-1}</m>,
		and we get <m>m_j</m> such terms for each eigenvalue because there are <m>m_j</m> linear factors involving that eigenvalue to end up as the <q>remaining factor</q> in our choices.
		Collecting these like terms,
		the coefficient on the term involving <m>\lambda^{n-1}</m> is
		<me>
			- m_1 \lambda_1 - m_2 \lambda_2 - \dotsb - m_\ell \lambda_\ell
			= - \trace A
		</me>.
		</p>

	</proof>

</theorem>


</subsection>

</section>
