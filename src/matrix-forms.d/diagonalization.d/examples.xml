<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2021 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-diagonalization-examples">

<title>Examples</title>

<assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-diagonalization-examples-diag-proc" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-diagonalization-examples-diag-proc" /></em></li>
<li><xref ref="subsection-diagonalization-examples-determining-diag" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-diagonalization-examples-determining-diag" /></em></li>
<li><xref ref="subsection-diagonalization-examples-different" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-diagonalization-examples-different" /></em></li>
</ul></p></assemblage>

<subsection xml:id="subsection-diagonalization-examples-diag-proc">
<title>Carrying out the diagonalization procedure</title>

<p> Let's start with some examples from <xref ref="worksheet-diagonalization" />. </p>

<example><title>A diagonalizable matrix</title><statement>
	<p>
	From <xref ref="activity-diagonalization-diag-proc" />.
	We want to compute a basis for each eigenspace,
	using the same method as in the examples of <xref ref="section-eigen-values-vectors-examples" />.
	First, we form the matrix
	<me>
		\lambda I - A = \begin{bmatrix}
			\lambda+1 \amp -9 \amp 0 \\
			0 \amp \lambda-2 \amp 0 \\
			0 \amp 3 \amp \lambda+1
		\end{bmatrix},
	</me>
	and compute its determinant,
	<me>
		c_A(\lambda)
		= \det(\lambda I - A)
		= (\lambda+1)\bigl[(\lambda-2)(\lambda+1) - 0\cdot 3 \bigr]
		= (\lambda+1)^2(\lambda-2),
	</me>
	to obtain the characteristic polynomial of <m>A</m>.
	The eigenvalues are <m>\lambda_1 = -1</m> and <m>\lambda_2 = 2</m>.
	The first eigenvalue is repeated as a root of the characteristic polynomial,
	while the second eigenvalue is not,
	so we have algebraic multiplicities <m>m_1 = 2</m> for <m>\lambda_1</m> and <m>m_2 = 1</m> for <m>\lambda_2</m>.
	Therefore, we will need two linearly independent eigenvectors from <m>E_{\lambda_1}(A)</m> and one more from <m>E_{\lambda_2}(A)</m>.
	</p><p>
	We determine eigenspace bases by row reducing, assigning parameters, and extracting basis vectors.
	Here are the results for this matrix:
	<md><mrow>
		(-1)I - A \amp=
		\left[\begin{array}{rrr}
			0 \amp -9 \amp 0 \\
			0 \amp -3 \amp 0 \\
			0 \amp 3 \amp 0
		\end{array}\right]
		\amp \amp\rowredarrow \amp
		\amp\begin{bmatrix}
			0 \amp 1 \amp 0 \\
			0 \amp 0 \amp 0 \\
			0 \amp 0 \amp 0
		\end{bmatrix}
	</mrow></md>
	<me>
		\implies \qquad
		E_{\lambda_1}(A) =
		\Span\left\{ \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix} \right\},
	</me>
	<md><mrow>
		2I - A \amp=
		\left[\begin{array}{rrr}
			3 \amp -9 \amp 0 \\
			0 \amp 0 \amp 0 \\
			0 \amp 3 \amp 3
		\end{array}\right]
		\amp \amp\rowredarrow \amp
		\amp\begin{bmatrix}
			1 \amp 0 \amp 3 \\
			0 \amp 1 \amp 1 \\
			0 \amp 0 \amp 0
		\end{bmatrix}
	</mrow></md>
	<me>
		\implies \qquad
		E_{\lambda_2}(A) = \Span\left\{ \left[\begin{array}{r}-3 \\ -1 \\ 1\end{array}\right] \right\}.
	</me>
	Notice that <m>\dim\bbrac{E_{\lambda_1}(A)} = 2</m>,
	so geometric multiplicity equals algebraic multiplicity for <m>\lambda_1</m>.
	Also, <m>\dim\bbrac{E_{\lambda_2}(A)} = 1</m>,
	so again geometric multiplicity equals algebraic multiplicity for <m>\lambda_2</m>.
	</p><p>
	Let's pause to consider the result for eigenvalue <m>\lambda_2</m>.
	We should have expected the result for the geometric multiplicity of eigenvalue <m>\lambda_2</m> from the relationship between algebraic and geometric multiplicites stated in
	<xref ref="subsection-diagonalization-concepts-diagonalizable" />.
	If we believe that a geometric multiplicity can never be greater than the corresponding algebraic multiplicity,
	then eigenspace <m>E_{\lambda_2}(A)</m> in this example could never have dimension greater than <m>1</m>.
	On the other hand,
	an eigenspace should never have dimension <m>0</m> because the definition of eigenvalue requires the existence of nonzero eigenvectors.
	So this forces the dimension of <m>E_{\lambda_2}(A)</m> to be <m>1</m>,
	without actually checking.
	</p><p>
	Returning to our procedure,
	we can see by inspection that the eigenspace basis vector for <m>\lambda_2</m> is linearly independent from the ones for <m>\lambda_1</m>,
	so when we form the transition matrix
	<me> P = \left[\begin{array}{rrr} 1 \amp 0 \amp -3 \\ 0 \amp 0 \amp -1 \\ 0 \amp 1 \amp 1 \end{array}\right], </me>
	it will be invertible because its columns are linearly independent.
	And we can determine the diagonal form matrix <m>\inv{P}AP</m> <em>without</em> calculating <m>\inv{P}</m>,
	because its diagonal entries should be precisely the eigenvalues of <m>A</m>,
	with the same multiplicities and order as the corresponding columns of <m>P</m>.
	In this case,
	<me> \inv{P}AP = \left[\begin{array}{rrr} -1 \amp 0 \amp 0 \\ 0 \amp -1 \amp 0 \\ 0 \amp 0 \amp 2 \end{array}\right]. </me>
	Finally,
	note that we could have analyzed the eigenvalues in the opposite order,
	in which case we would have formed transition matrix
	<me> Q = \left[\begin{array}{rrr} -3 \amp 1 \amp 0 \\ -1 \amp 0 \amp 0 \\ 1 \amp 0 \amp 1 \end{array}\right], </me>
	obtaining diagonal form matrix
	<me> \inv{Q}AQ = \left[\begin{array}{rrr} 2 \amp 0 \amp 0 \\ 0 \amp -1 \amp 0 \\ 0 \amp 0 \amp -1 \end{array}\right]. </me>
	</p>
</statement></example>

<example><title>A non-diagonalizable matrix</title><statement>
	<p>
	From <xref ref="activity-diagonalization-non-diag-ex" />.
	This matrix is upper triangular,
	so we can see directly that the eigenvalues are <m>\lambda_1 = -1</m> with algebraic multiplicity <m>2</m>,
	and <m>\lambda_2 = 2</m> with algebraic multiplicity <m>1</m>.
	Analyze the eigenspaces:
	<md><mrow>
		(-1)I - A \amp=
		\left[\begin{array}{rrr}
			0 \amp -1 \amp 0 \\
			0 \amp 0 \amp 0 \\
			0 \amp 0 \amp -3
		\end{array}\right]
		\amp \amp\rowredarrow \amp
		\amp\begin{bmatrix}
			0 \amp 1 \amp 0 \\
			0 \amp 0 \amp 1 \\
			0 \amp 0 \amp 0
		\end{bmatrix}
	</mrow></md>
	<me>
		\implies \qquad
		E_{\lambda_1}(A) =
		\Span\left\{ \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \right\},
	</me>
	<md><mrow>
		2I - A \amp=
		\left[\begin{array}{rrr}
			3 \amp -1 \amp 0 \\
			0 \amp 3 \amp 0 \\
			0 \amp 0 \amp 0
		\end{array}\right]
		\amp \amp\rowredarrow \amp
		\amp\begin{bmatrix}
			1 \amp 0 \amp 0 \\
			0 \amp 1 \amp 0 \\
			0 \amp 0 \amp 0
		\end{bmatrix}
	</mrow></md>
	<me>
		\implies \qquad
		E_{\lambda_2}(A) = \Span\left\{ \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \right\}.
	</me>
	We could have stopped after our analysis of <m>\lambda_1</m>,
	since its geometric multiplicity is only <m>1</m>,
	whereas we needed it to be equal to the algebraic multiplicity <m>2</m>.
	Since we cannot obtain enough linearly independent eigenvectors from these two eigenspaces to fill out a <m>3 \times 3</m> transition matrix <m>P</m>,
	matrix <m>A</m> is <em>not</em> diagonalizable.
	</p>
</statement></example>

<remark><p>
	The matrices in the two examples above had the same eigenvalues with the same algebraic multiplicities,
	but one matrix was diagonalizable and the other was not.
	The difference was in the geometric multiplicities of the eigenvalues,
	which plays a crucial role in determining whether a matrix is diagonalizable.
</p></remark>

</subsection>

<subsection xml:id="subsection-diagonalization-examples-determining-diag">
<title>Determining diagonalizability from multiplicities</title>

<p>
Here is an example where we only concern ourselves with the question of whether a matrix is <term>diagonalizable</term>,
without attempting to build a transition matrix <m>P</m>.
</p>

<p>
Is
<me>
	A =
	\left[\begin{array}{rrrr}
		-1 \amp 0 \amp -12 \amp 0 \\
		0 \amp 1 \amp -8 \amp 0 \\
		0 \amp 0 \amp 5 \amp 0 \\
		4 \amp 0 \amp 4 \amp 3
	\end{array}\right]
</me>
diagonalizable?
Compute the characteristic polynomial:
<md><mrow>
	\det(\lambda I - A) \amp =
	\begin{vmatrix}
		\lambda+1 \amp 0 \amp 12 \amp 0 \\
		0 \amp \lambda-1 \amp 8 \amp 0 \\
		0 \amp 0 \amp \lambda-5 \amp 0 \\
		-4 \amp 0 \amp -4 \amp \lambda-3
	\end{vmatrix}
	\\
	\amp = (\lambda-5)(\lambda+1)(\lambda-1)(\lambda-3) \text{.}
</mrow></md>
So the eigenvalues are <m>\lambda = -1,1,3,5</m>,
each with algebraic multiplicity <m>1</m>.
But an eigenspace must contain nonzero eigenvectors,
so eigenvalues always have geometric multiplicity <em>at least</em> <m>1</m>.
Since we will be able to obtain an eigenvector from each of the four eigenvalues,
we'll be able to fill the four columns of the transition matrix <m>P</m> with linearly independent eigenvectors.
Therefore,
<m>A</m> is diagonalizable.
</p>

<remark><p>
	The analysis used in the above example only works for eigenvalues of algebraic multiplicity <m>1</m>.
	If an eigenvalue has algebraic multiplicity greater than <m>1</m>,
	then we still must row reduce <m>\lambda I - A</m> to determine the geometric multiplicity of the eigenvalue.
	However, if all we are concerned with is the question of <term>diagonalizability</term>,
	then we don't need to carry out the full procedure <mdash />
	we can stop row reducing as soon as we can see how many parameters will be required,
	since this tells us the dimension of the eigenspace.
</p></remark>

</subsection>


<subsection xml:id="subsection-diagonalization-examples-different">
<title>A different kind of example</title>

<p>
Is
<me> A = \left[\begin{array}{rr} 0 \amp -1 \\ 1 \amp 0 \end{array}\right] </me>
diagonalizable? Compute the characteristic polynomial:
<me>
	\det(\lambda I - A)
	= \left\lvert\begin{array}{rr} \lambda \amp 1 \\ -1 \amp \lambda \end{array}\right\rvert
	= \lambda^2 + 1.
</me>
But <m>\lambda^2+1=0</m> does not have real solutions,
so <m>A</m> does not have eigenvalues, and cannot be diagonalizable.
</p>

<paragraphs><title>A look ahead</title><p>
	<!-- TODO rewrite this after notes are extended to complex vector spaces -->
	In future studies in linear algebra you may study matrix forms in more detail,
	in which case you will likely work with <term>complex</term> vector spaces,
	where scalars are allowed to be <em>both</em> real and imaginary numbers.
	In that context,
	the matrix in the last example above <em>does</em> have eigenvalues,
	and in fact can be diagonalized.
</p></paragraphs>

</subsection>

</section>
