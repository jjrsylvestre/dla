<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************
© 2016–2023 Jeremy Sylvestre

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the appendix entitled “GNU Free
Documentation License” that appears in the output document of this
PreTeXt source code. All trademarks™ are the registered® marks of their
respective owners.
*********************************************************************-->


<section xml:id="section-matrix-ops-examples">

<title>Examples</title>

<assemblage><title>In this section</title><p><ul>
<li><xref ref="subsection-matrix-ops-examples-basics" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-matrix-ops-examples-basics" /></em></li>
<li><xref ref="subsection-matrix-ops-examples-mult" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-matrix-ops-examples-mult" /></em></li>
<li><xref ref="subsection-matrix-ops-examples-combined" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-matrix-ops-examples-combined" /></em></li>
<li><xref ref="subsection-matrix-ops-examples-sys-matrix-eqns" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-matrix-ops-examples-sys-matrix-eqns" /></em></li>
<li><xref ref="subsection-matrix-ops-examples-transpose" /><nbsp /><nbsp />
<em><xref text="title" ref="subsection-matrix-ops-examples-transpose" /></em></li>
</ul></p></assemblage>


<subsection xml:id="subsection-matrix-ops-examples-basics">
<title>Basic matrix operations</title>

<p>
Here are some basic examples of matrix addition, subtraction, and scalar multiplication.
For subtraction, watch out for double negatives!
</p>

<example xml:id="example-matrix-ops-add"><title>Matrix addition</title><p><me>
	\left[\begin{array}{rr} 1 \amp -2 \\ 3 \amp 4 \\ -5 \amp 6 \end{array}\right]
	+ \left[\begin{array}{rr} 0 \amp 1 \\ 1 \amp -2 \\ 11 \amp 0 \end{array}\right]
	= \begin{bmatrix} 1+0 \amp -2+1 \\ 3+1 \amp 4+(-2) \\ -5+11 \amp 6+0 \end{bmatrix}
	= \left[\begin{array}{rr} 1 \amp -1 \\ 4 \amp 2 \\ 6 \amp 6 \end{array}\right]
</me></p></example>

<example xml:id="example-matrix-ops-subtr"><title>Matrix subtraction</title><p><md><mrow>
	\left[\begin{array}{rrr} 1 \amp -2 \amp 3 \\ 0 \amp -4 \amp -5 \end{array}\right]
	- \left[\begin{array}{rrr} 0 \amp 1 \amp 1 \\ -2 \amp 11 \amp -1 \end{array}\right]
	\amp = \begin{bmatrix} 1-0 \amp -2-1 \amp 3-1 \\ 0-(-2) \amp -4-11 \amp -5-(-1) \end{bmatrix}
	</mrow><mrow>
	\amp = \left[\begin{array}{rrr} 1 \amp -3 \amp 2 \\ 2 \amp -15 \amp -4 \end{array}\right]
</mrow></md></p></example>

<example xml:id="example-matrix-ops-scalar-mul"><title>Scalar multiplication of a matrix</title><p><me>
		(-5)\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
		= \left[\begin{array}{rr} -5 \amp 10 \\ 15 \amp -20\end{array}\right]
</me></p></example>

</subsection>


<subsection xml:id="subsection-matrix-ops-examples-mult">
<title>Matrix multiplication</title>

<example xml:id="example-matrix-ops-detailed-mult">
	<title>A detailed multiplication example</title>
	<p>
	Let's compute the matrix product <m>AB</m>, for
	<md><mrow>
		A \amp = \left[\begin{array}{rr}
			3 \amp -2 \\
			1 \amp 0 \\
			-4 \amp 5\
		\end{array}\right],
		\amp
		B \amp = \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right].
	</mrow></md>
	Notice that the sizes of <m>A</m> (<m>3\times 2</m>) and <m>B</m> (<m>2\times 2</m>) are compatible for multiplication in the order <m>AB</m>,
	and that the result will be size <m>3\times 2</m>.
	First let's multiply <m>A</m> onto the columns of <m>B</m>.
	<md>
		<mrow>
			\left[\begin{array}{rr} 3 \amp -2 \\ 1 \amp 0 \\ -4 \amp 5 \end{array}\right]
			\left[\begin{array}{r} 1 \\ -3 \end{array}\right]
			=
			\begin{bmatrix}
				3\cdot 1 + (-2)\cdot(-3) \\
				1 \cdot 1 + 0\cdot(-3) \\
				-4\cdot 1 + 5\cdot(-3)
			\end{bmatrix}
			= \left[\begin{array}{r} 9 \\ 1 \\ -19 \end{array}\right]
		</mrow>
		<mrow>
			\left[\begin{array}{rr} 3 \amp -2 \\ 1 \amp 0 \\ -4 \amp 5 \end{array}\right]
			\left[\begin{array}{r} -2 \\ 4 \end{array}\right]
			=
			\begin{bmatrix}
				3\cdot (-2) + (-2)\cdot 4 \\
				1 \cdot (-2) + 0\cdot 4 \\
				-4\cdot (-2) + 5\cdot 4
			\end{bmatrix}
			= \left[\begin{array}{r} -14 \\ -2 \\ 28 \end{array}\right]
		</mrow>
	</md>
	Combining these two computations, we get
	<me>
		A B =
		\left[\begin{array}{rr} 3 \amp -2 \\ 1 \amp 0 \\ -4 \amp 5 \end{array}\right]
		\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
		=
		\left[\begin{array}{rr} 9 \amp -14 \\ 1 \amp -2 \\ -19 \amp 28 \end{array}\right].
	</me>
	With some practise at matrix multiplication,
	you should be able to compute a product <m>A B</m> directly without doing separate computations for each column of the second matrix.
	</p><p>
	In this matrix multiplication example,
	notice that it does not make sense to even consider the possibility that <m>B A = A B</m> because the sizes of <m>B</m> and <m>A</m> are not compatible for multiplication in the order <m>B A</m>,
	and so <m>B A</m> is undefined!
	<aside>
		<title>Check your understanding</title>
		<p>	Is it <em>never</em> true that <m> B A = A B</m>?
		It should be obvious that it will be true if <m>A</m> is a square zero matrix and <m>B</m> is a square matrix of the same size.
		Can you come up with an example of <m>2\times 2</m> matrices <m>A</m> and <m>B</m> where neither is the zero matrix,
		and <m>B A = A B</m> is true?</p>
	</aside>
	</p>
</example>

<example><title>Matrix powers</title><p>
	Since powers of matrices only work for <em>square matrices</em>,
	the power <m>A^2</m> is undefined for the <m>3\times 2</m> matrix <m>A</m> in the previous matrix multiplication example.
	But we can compute <m>B^2</m> for the <m>2\times 2</m> matrix <m>B</m> from that example.
	<md>
		<mrow>
			B^2 = B B \amp =
			\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
		</mrow>
		<mrow>
			\amp =
			\left[\begin{array}{rr}
				1\cdot 1 + (-2)(-3) \amp 1(-2) + (-2)\cdot 4 \\
				-3\cdot 1 + 4(-3) \amp (-3)(-2) + 4\cdot 4
			\end{array}\right]
			= \left[\begin{array}{rr} 7 \amp -10 \\ -15 \amp 22 \end{array}\right]
		</mrow>
	</md>
	To compute <m>B^3</m>, we can compute either of
	<md>
		<mrow>
			B^3 = B B B = (B B) B
			\amp= B^2 B
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 7 \amp -10 \\ -15 \amp 22 \end{array}\right]
			\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
		</mrow><mrow>
			\amp=
			\left[\begin{array}{rr}
				7\cdot1 + (-10)(-3) \amp 7(-2) + (-10)\cdot 4 \\
				-15\cdot 1 + 22(-3) \amp -15(-2) + 22\cdot 4
			\end{array}\right]
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 37 \amp -54 \\ -81 \amp 118 \end{array}\right],
		</mrow>
	<intertext>or</intertext>
		<mrow>
			B^3 = B B B = B (B B)
			\amp= BB^2
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left[\begin{array}{rr} 7 \amp -10 \\ -15 \amp 22 \end{array}\right]
		</mrow><mrow>
			\amp=
			\left[\begin{array}{rr}
				1\cdot 7 + (-2)(-15) \amp 1(-10) + (-2)\cdot 22 \\
				(-3)\cdot 7 + 4(-15) \amp -3(-10) + 4\cdot 22
			\end{array}\right]
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 37 \amp -54 \\ -81 \amp 118 \end{array}\right],
		</mrow>
	</md>
	and the result is the same.
</p></example>

</subsection>

<subsection xml:id="subsection-matrix-ops-examples-combined">
<title>Combining operations</title>

<example><title>Computing matrix formulas involving a combination of operations</title><p>
	Let's compute both <m>A (B + k C)</m> and <m>A B + k (A C)</m>, for
	<md><mrow>
		A \amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right], \amp
		B \amp= \left[\begin{array}{rr} 0 \amp 2 \\ 1 \amp -1 \end{array}\right], \amp
		C \amp= \left[\begin{array}{rr} 5 \amp 5 \\ -2 \amp -2 \end{array}\right], \amp
		k \amp= 3.
	</mrow></md>
	Keep in mind that <em>operations inside brackets should be performed first</em>,
	and as usual multiplication (both matrix and scalar) should be performed before addition
	(unless there are brackets to tell us otherwise).
	<md>
		<mrow>
			A (B + k C)
			\amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left(
				\left[\begin{array}{rr} 0 \amp 2 \\ 1 \amp -1 \end{array}\right]
				+ 3\left[\begin{array}{rr} 5 \amp 5 \\ -2 \amp -2 \end{array}\right]
			\right)
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left(
				\left[\begin{array}{rr} 0 \amp 2 \\ 1 \amp -1 \end{array}\right]
				+ \left[\begin{array}{rr} 15 \amp 15 \\ -6 \amp -6 \end{array}\right]
			\right)
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left[\begin{array}{rr} 15 \amp 17 \\ -5 \amp -7  \end{array}\right]
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 25 \amp 31 \\ -65 \amp -79 \end{array}\right]
		</mrow>
	</md>
	<md>
		<mrow>
			A B + k (A C)
			\amp= \left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
			\left[\begin{array}{rr} 0 \amp 2 \\ 1 \amp -1 \end{array}\right]
			+ 3\left(
				\left[\begin{array}{rr} 1 \amp -2 \\ -3 \amp 4 \end{array}\right]
				\left[\begin{array}{rr} 5 \amp 5 \\ -2 \amp -2  \end{array}\right]
			\right)
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} -2 \amp 4 \\ 4 \amp -10 \end{array}\right]
			+ 3 \left[\begin{array}{rr} 9 \amp 9 \\ -23 \amp -23 \end{array}\right]
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} -2 \amp 4 \\ 4 \amp -10 \end{array}\right]
			+ \left[\begin{array}{rr} 27 \amp 27 \\ -69 \amp -69  \end{array}\right]
		</mrow><mrow>
			\amp= \left[\begin{array}{rr} 25 \amp 31 \\ -65 \amp -79  \end{array}\right]
		</mrow>
	</md>
</p></example>

<p>
Hopefully you're not surprised that we got the same final result for both the formulas <m>A (B + k C)</m> and <m>A B + k (A C)</m>.
From our familiar rules of algebra, we expect to be able to multiply <m>A</m> inside the brackets in the first expression,
and then rearrange the order of multiplication by <m>A</m> and by <m>k</m>.
However, we need to be careful <mdash />
our <q>familiar</q> rules of algebra come from operations with <em>numbers</em>,
and matrix algebra involves operations with <em>matrices</em>:
addition, subtraction, and <em>two different</em> kinds of multiplication, scalar and matrix.
We should not <em>blindly</em> expect all of our <q>familiar</q> rules of algebra to apply to matrix operations.
We've already seen that the matrix version of the familiar rule <m>b a = a b</m> is <em>not</em> true for matrix multiplication!
In
<xref ref="subsection-matrix-ops-theory-algebra" />,
we list the rules of algebra that <em>are</em> valid for matrix operations
(which is <em>most</em> of our familiar rules from the algebra of numbers),
and for some of the rules, in that same subsection we verify that they are indeed valid for matrices.
</p>

</subsection>

<subsection xml:id="subsection-matrix-ops-examples-sys-matrix-eqns">
<title>Linear systems as matrix equations</title>

<subsubsection>
<title>A first example</title>

<example><title>A system as a matrix equation</title>
	<p>
	Let's again consider the system from
	<xref ref="activity-matrix-ops-system-to-matrix-mult-sys-to-eq-cols" text="type-local"/> of
	<xref ref="activity-matrix-ops-system-to-matrix-mult" />.
	To solve, we row reduce the associated augmented matrix to RREF as usual.
	<md><mrow>
		\amp
		\left[\begin{array}{rrr|r}
			 1 \amp -3 \amp -1 \amp -4 \\
			-2 \amp  7 \amp  2 \amp  9 
		\end{array}\right]
		\amp\amp\rowredarrow\amp
		\amp
		\left[\begin{array}{rrr|r}
			1 \amp 0 \amp -1 \amp -1 \\
			0 \amp 1 \amp  0 \amp  1
		\end{array}\right]
	</mrow></md>
	Variable <m>x_3</m> is free, so assign a parameter <m>x_3 = t</m>.
	Then we can solve to obtain the general solution is parametric form,
	<md><mrow>
		x_1 \amp= -1 + t, \amp x_2 \amp= 1, \amp x_3 \amp= t.
	</mrow></md>
	Let's check a couple of particular solutions against the matrix <em>equation</em> <m>A\uvec{x}=\uvec{b}</m> that represents the system.
	Recall that for this system, <m>\uvec{x}</m> is the <m>3\times 1</m> column vector that contains the variables <m>x_1,x_2,x_3</m>.
	The particular solutions associated to parameter values <m>t=0</m> and <m>t=3</m> are
	<md>
		<mrow>
			t = 0 \amp\colon  \amp x_1 \amp= -1, \amp x_2 \amp= 1, \amp x_3 \amp= 0;
		</mrow>
	<intertext>and</intertext>
		<mrow>
			t = 3 \amp\colon \amp x_1 \amp=  2, \amp x_2 \amp=  1, \amp x_3 \amp= 3.
		</mrow>
	</md>
	Let's collect the <m>t=0</m> solution values into the vector <m>\uvec{x}</m> and check <m>A\uvec{x}</m> versus <m>\uvec{b}</m>:
	<me>
		\text{LHS}
		= A\uvec{x}
		= \left[\begin{array}{rrr}
			 1 \amp -3 \amp -1 \\
			-2 \amp  7 \amp  2 
		\end{array}\right]
		\left[\begin{array}{r} -1 \\ 1 \\ 0 \end{array}\right]
		= \begin{bmatrix} -1 - 3 + 0 \\ 2 + 7 + 0 \end{bmatrix}
		= \left[\begin{array}{r} -4 \\ 9 \end{array}\right]
		= \uvec{b}
		= \text{RHS}.
	</me>
	So the solution to the linear system we got by row reducing did indeed give us a vector solution <m>\uvec{x}</m> to the matrix equation
	<m> A \uvec{x} = \uvec{b}</m>.
	Let's similarly check the <m>t=3</m> solution, as in
	<xref ref="activity-matrix-ops-system-to-matrix-mult-check-sol" text="type-local" />
	of <xref ref="activity-matrix-ops-system-to-matrix-mult"/>:
	<me>
		\text{LHS}
		= A\uvec{x}
		= \left[\begin{array}{rrr}
			 1 \amp -3 \amp -1 \\
			-2 \amp  7 \amp  2 
		\end{array}\right]
		\left[\begin{array}{r} 2 \\ 1 \\ 3 \end{array}\right]
		= \begin{bmatrix} 2 - 3 - 3 \\ -4 + 7 + 6 \end{bmatrix}
		= \left[\begin{array}{r} -4 \\ 9 \end{array}\right]
		= \uvec{b}
		= \text{RHS}.
	</me>
	Again, our system solution gives us a solution to the matrix equation.
	</p>
	<aside><title>Check your understanding</title><p>
		Carry out the same verification as in this example for the <em>general</em> solution to the system,
		with the parameter <m>t</m> left variable.
	</p></aside>
</example>

</subsubsection>

<subsubsection xml:id="subsubsection-matrix-ops-examples-sys-sol-vector-forms">
<title>Expressing system solutions in vector form</title>

<p>
We may use matrices and matrix algebra to express the solutions to solutions as column vectors.
In particular, we can expand solutions involving parameters into a
<term>linear combination</term><idx><h>linear</h><h>combination</h></idx>
of column vectors.
Expressing solutions this way allows us to see the effect of each parameter on the system.
</p><p>
Let's re-examine the systems in the examples from <xref ref="section-row-red-examples" /> as matrix equations,
and express their solutions in vector form.
</p>

<example xml:id="example-matrix-ops-sys-sol-vector-forms-unique">
	<title>Solutions in vector form: one unique solution</title>
	<p>
	The system from <xref ref="activity-row-red-system-unique-sol" /> can be expressed in the form <m>A\uvec{x} = \uvec{b}</m> for
	<md><mrow>
		A \amp =
		\left[\begin{array}{rrr}
			2 \amp 0 \amp -2 \\
			1 \amp -1 \amp 0 \\
			4 \amp -2 \amp -3
		\end{array}\right],
		\amp
		\uvec{x} \amp = \begin{bmatrix} x \\ y \\ z \end{bmatrix},
		\amp
		\uvec{b} \amp = \begin{bmatrix} 4 \\ 3 \\ 7 \end{bmatrix}.
	</mrow></md>
	We solved this system in
	<xref ref="example-row-red-solving-unique" />
	and determined that it had one unique solution,
	<m>x = 5</m>, <m>y = 2</m>, and <m>z = 3</m>.
	In vector form, we write this solution as
	<me>
		\uvec{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}
		= \left[\begin{array}{r} 5 \\ 2 \\ 3 \end{array}\right].
	</me>
	</p>
</example>

<example xml:id="example-matrix-ops-sys-sol-vector-forms-infinite">
	<title>Solutions in vector form: an infinite number of solutions</title>
	<p>
	The system from <xref ref="activity-row-red-system-infinite-sol" /> can be expressed in the form <m>A \uvec{x} = \uvec{b}</m> for
	<md><mrow>
		A \amp =
		\left[\begin{array}{rrr}
			3 \amp 6 \amp 5 \\
			2 \amp 4 \amp 3 \\
			3 \amp 6 \amp 6
		\end{array}\right],
		\amp
		\uvec{x} \amp = \begin{bmatrix} x \\ y \\ z \end{bmatrix},
		\amp
		\uvec{b} \amp = \left[\begin{array}{r} -9 \\ -5 \\ -12 \end{array}\right].
	</mrow></md>
	We solved this system in
	<xref ref="example-row-red-solving-infinite" />,
	and determined that it had an infinite number of solutions.
	We expressed the general solution to the system using parametric equations
	<md><mrow>
		x \amp= 2-2t, \amp y \amp= t, \amp z = -3,
	</mrow></md>
	In vector form, we expand this solution as
	<me>
		\uvec{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}
		= \begin{bmatrix}2-2t\\t\\-3\end{bmatrix}
		= \begin{bmatrix}2-2t\\0+t\\-3+0t\end{bmatrix}
		= \left[\begin{array}{r}2\\0\\-3\end{array}\right]
		+ \left[\begin{array}{r}-2t\\t\\0t\end{array}\right]
		= \left[\begin{array}{r}2\\0\\-3\end{array}\right]
		+ t\left[\begin{array}{r}-2\\1\\0\end{array}\right].
	</me>
	Notice how the solution is the sum of a <term>constant part</term>
	<me> \left[\begin{array}{r}2\\0\\-3\end{array}\right] </me>
	and a <term>variable part</term>
	<me> t\left[\begin{array}{r}-2\\1\\0\end{array}\right]. </me>
	Further notice how the constant part is a particular solution to the system <mdash />
	it is the <q>initial</q> particular solution associated to the parameter value <m>t=0</m>.
	</p>
</example>

<example xml:id="example-matrix-ops-sys-sol-vector-forms-homog">
	<title>Solutions in vector form: a homogenous system</title>
	<p>
	The system from <xref ref="activity-row-red-system-homog-sol" /> is homogeneous,
	so it can be expressed in the form <m>A\uvec{x} = \zerovec</m> for
	<md><mrow>
		A \amp =
		\left[\begin{array}{rrrr}
			3 \amp 6 \amp -8 \amp 13 \\
			1 \amp 2 \amp -2 \amp 3 \\
			2 \amp 4 \amp -5 \amp 8
		\end{array}\right],
		\amp
		\uvec{x} \amp = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix},
	</mrow></md>
	where <m>\zerovec</m> is the <m>3 \times 1</m> zero column vector. We solved this system in
	<xref ref="example-row-red-solving-homog" />,
	and determined that it had an infinite number of solutions.
	We expressed the general solution to the system using parametric equations
	<md><mrow> x_1 \amp= -2s + t, \amp x_2 \amp= s, \amp x_3 \amp= 2t, \amp x_4 \amp= t </mrow></md>.
	In vector form, we expand this solution as
	<me>
		\uvec{x} = \begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}
		= \begin{bmatrix}-2s+t\\s\\2t\\t\end{bmatrix}
		= \begin{bmatrix}-2s+t\\s+0t\\0s+2t\\0s+t\end{bmatrix}
		= \left[\begin{array}{r}-2s\\s\\0s\\0s\end{array}\right]
		+ \left[\begin{array}{r}t\\0t\\2t\\t\end{array}\right]
		= s\left[\begin{array}{r}-2\\1\\0\\0\end{array}\right]
		+ t\left[\begin{array}{r}1\\0\\2\\1\end{array}\right].
	</me>
	This time, the solution is a sum of <em>two</em> variables parts,
	<md><mrow>
		\amp s\left[\begin{array}{r}-2\\1\\0\\0\end{array}\right]
		\amp \amp\text{and}\amp
		\amp t\left[\begin{array}{r}1\\0\\2\\1\end{array}\right]
	</mrow></md>,
	since there are two parameters.
	And there is <em>no</em> constant part to the general solution,
	because if we set both parameters to zero we obtain the trivial solution <m>\uvec{x} = \zerovec</m>.
	A homogeneous system will always work out this way.
	(So it would be more accurate to say that the general solution to the system from
	<xref ref="activity-row-red-system-homog-sol" />
	has <em>trivial</em> constant part, instead of saying it has <em>no</em> constant part.)
	</p>
</example>

<example xml:id="example-matrix-ops-sys-sol-vector-forms-corresponding-homog">
	<title>Solutions in vector form: patterns in homogeneous/nonhomogeneous systems</title>
	<p>
	In
	<xref ref="example-row-red-solving-corresponding-homog" />,
	we solved a homogenous system <m>A\uvec{x} = \zerovec</m> with
	<md><mrow>
		A \amp =
		\left[\begin{array}{rrr}
			3 \amp 6 \amp 5 \\
			2 \amp 4 \amp 3 \\
			3 \amp 6 \amp 6
		\end{array}\right],
		\amp
		\uvec{x} \amp = \begin{bmatrix} x \\ y \\ z \end{bmatrix},
	</mrow></md>
	and found an infinite number of solutions,
	with general solution expressed parametrically as
	<md><mrow> x \amp = -2t, \amp y \amp = t, \amp z \amp = 0. </mrow></md>
	In vector form, we express this as
	<me>
		\uvec{x} = \begin{bmatrix}x\\y\\z\end{bmatrix}
		= \left[\begin{array}{r}-2t\\t\\0\end{array}\right]
		= \left[\begin{array}{r}-2t\\t\\0t\end{array}\right]
		= t\left[\begin{array}{r}-2\\1\\0\end{array}\right]
	</me>.
	This homogeneous system has the same coefficient matrix as in
	<xref ref="example-matrix-ops-sys-sol-vector-forms-infinite" />
	above, so it is not surprising that their general solutions are related.
	In particular, notice that both systems <em>have the same variable part</em>,
	but that the nonhomogeneous system from
	<xref ref="example-matrix-ops-sys-sol-vector-forms-infinite" />
	has a non-trivial constant part.
	</p>
	<aside><title>Compare</title><p>
		with the discussion at the end of
		<xref ref="example-row-red-solving-corresponding-homog" />.
		</p>
	</aside>
</example>

</subsubsection>

</subsection>


<subsection xml:id="subsection-matrix-ops-examples-transpose">
<title>Transpose</title>

<example><title>Computing transposes</title><p>
	Let's compute some transposes.
	<md>
		<mrow>
			A \amp = \begin{bmatrix} 1 \amp 2 \amp 3\\4 \amp 5 \amp 6 \end{bmatrix}
			\amp
			B \amp =
			\left[\begin{array}{rrr}
				-1 \amp 2 \amp 3 \\
				5 \amp 0 \amp 4 \\
				6 \amp 7 \amp 1
			\end{array}\right]
			\amp
			C \amp =
			\begin{bmatrix}
				0 \amp 1 \amp 2 \\
				1 \amp 0 \amp 3 \\
				2 \amp 3 \amp 0
			\end{bmatrix}
		</mrow><mrow>
			\utrans{A} \amp = \begin{bmatrix} 1 \amp 4\\2 \amp 5 \\ 3 \amp 6 \end{bmatrix} \amp
			\utrans{B} \amp =\
			\left[\begin{array}{rrr}
				-1 \amp 5 \amp 6 \\
				2 \amp 0 \amp 7 \\
				3 \amp 4 \amp 1
			\end{array}\right]
			\amp
			\utrans{C} \amp =
			\begin{bmatrix}
				0 \amp 1 \amp 2 \\
				1 \amp 0 \amp 3 \\
				2 \amp 3 \amp 0
			\end{bmatrix}
		</mrow>
	</md>
	The matrix <m>A</m> is size <m>2 \times 3</m>,
	so when we turn rows into columns to compute <m>\utrans{A}</m>,
	we end up with a <m>3\times 2</m> result.
	Matrices <m>B</m> and <m>C</m> are square,
	so each of their transposes end up being the same size as the original matrix.
	But also, the numbers for the entries in <m>B</m> and <m>C</m> were chosen to emphasize some patterns in the transposes of square matrices.
	In interchanging rows and columns in <m>B</m>,
	notice how entries to the upper right of the main diagonal move to the <q>mirror image</q> position in the lower left of the main diagonal,
	and vice versa.
	So for square matrices,
	we might think of the transpose as <q>reflecting</q> entries in the main diagonal,
	while entries right on the main diagonal end up staying in place.
	Finally, we might consider this same <q>reflecting-in-the-diagonal</q> view of the transpose for <m>C</m>,
	except <m>C</m> has the <em>same</em> entries in corresponding <q>mirror image</q> entries on either side of the diagonal,
	and so we end up with <m>\utrans{C} = C</m>.
	</p>
</example>

</subsection>

</section>
